
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                  ARGS                  ‚îÇ PROFILE  ‚îÇ   USER    ‚îÇ VERSION ‚îÇ      START TIME      ‚îÇ       END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 19:40 CEST ‚îÇ                      ‚îÇ
‚îÇ delete  ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 19:49 CEST ‚îÇ 14 Sep 25 19:49 CEST ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 19:50 CEST ‚îÇ                      ‚îÇ
‚îÇ stop    ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 19:57 CEST ‚îÇ 14 Sep 25 19:58 CEST ‚îÇ
‚îÇ start   ‚îÇ --memory=2048 --cpus=2 --driver=docker ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 21:00 CEST ‚îÇ 14 Sep 25 21:02 CEST ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 21:03 CEST ‚îÇ 14 Sep 25 21:04 CEST ‚îÇ
‚îÇ service ‚îÇ kukuk-backend -n dev                   ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:23 CEST ‚îÇ 14 Sep 25 23:34 CEST ‚îÇ
‚îÇ service ‚îÇ kukuk-backend -n dev                   ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:38 CEST ‚îÇ                      ‚îÇ
‚îÇ stop    ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:42 CEST ‚îÇ 14 Sep 25 23:42 CEST ‚îÇ
‚îÇ service ‚îÇ kukuk-backend -n dev                   ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:49 CEST ‚îÇ                      ‚îÇ
‚îÇ start   ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:49 CEST ‚îÇ 14 Sep 25 23:50 CEST ‚îÇ
‚îÇ service ‚îÇ kukuk-backend -n dev                   ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 14 Sep 25 23:50 CEST ‚îÇ 15 Sep 25 00:20 CEST ‚îÇ
‚îÇ stop    ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 15 Sep 25 01:10 CEST ‚îÇ                      ‚îÇ
‚îÇ delete  ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 15 Sep 25 01:15 CEST ‚îÇ 15 Sep 25 01:15 CEST ‚îÇ
‚îÇ start   ‚îÇ --memory=4096 --cpus=2                 ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 15 Sep 25 01:15 CEST ‚îÇ 15 Sep 25 01:20 CEST ‚îÇ
‚îÇ start   ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:09 CEST ‚îÇ                      ‚îÇ
‚îÇ delete  ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:16 CEST ‚îÇ 17 Sep 25 05:16 CEST ‚îÇ
‚îÇ start   ‚îÇ --memory=8192 --cpus=4                 ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:16 CEST ‚îÇ                      ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:17 CEST ‚îÇ 17 Sep 25 05:26 CEST ‚îÇ
‚îÇ service ‚îÇ backend-service                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:28 CEST ‚îÇ                      ‚îÇ
‚îÇ service ‚îÇ backend-service                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 17 Sep 25 05:51 CEST ‚îÇ                      ‚îÇ
‚îÇ start   ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 20 Sep 25 02:49 CEST ‚îÇ                      ‚îÇ
‚îÇ start   ‚îÇ                                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 22:01 CEST ‚îÇ                      ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 22:03 CEST ‚îÇ 23 Sep 25 22:05 CEST ‚îÇ
‚îÇ service ‚îÇ frontend-service                       ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 22:14 CEST ‚îÇ                      ‚îÇ
‚îÇ service ‚îÇ frontend-service                       ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 23:03 CEST ‚îÇ 23 Sep 25 23:07 CEST ‚îÇ
‚îÇ service ‚îÇ backend-service --url                  ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 23:05 CEST ‚îÇ 23 Sep 25 23:12 CEST ‚îÇ
‚îÇ service ‚îÇ backend-service                        ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 23 Sep 25 23:08 CEST ‚îÇ                      ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                         ‚îÇ minikube ‚îÇ adziallas ‚îÇ v1.37.0 ‚îÇ 28 Sep 25 03:37 CEST ‚îÇ                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/09/23 22:03:10
Running on machine: DESKTOP-V17HJ83
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0923 22:03:10.700735    3786 out.go:360] Setting OutFile to fd 1 ...
I0923 22:03:10.700895    3786 out.go:413] isatty.IsTerminal(1) = true
I0923 22:03:10.700899    3786 out.go:374] Setting ErrFile to fd 2...
I0923 22:03:10.700903    3786 out.go:413] isatty.IsTerminal(2) = true
I0923 22:03:10.701136    3786 root.go:338] Updating PATH: /home/adziallas/.minikube/bin
I0923 22:03:10.701469    3786 out.go:368] Setting JSON to false
I0923 22:03:10.702656    3786 start.go:130] hostinfo: {"hostname":"DESKTOP-V17HJ83","uptime":15442,"bootTime":1758642349,"procs":52,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"3cd91cb8-1e04-4ccb-9cbb-7ca095cb9a11"}
I0923 22:03:10.702740    3786 start.go:140] virtualization:  guest
I0923 22:03:10.763213    3786 out.go:179] üòÑ  minikube v1.37.0 on Ubuntu 24.04 (amd64)
I0923 22:03:10.982626    3786 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0923 22:03:10.983214    3786 driver.go:421] Setting default libvirt URI to qemu:///system
I0923 22:03:10.988198    3786 notify.go:220] Checking for updates...
I0923 22:03:11.050727    3786 docker.go:123] docker version: linux-28.4.0:Docker Desktop 4.46.0 (204649)
I0923 22:03:11.050796    3786 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0923 22:03:12.072311    3786 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.02147822s)
I0923 22:03:12.073108    3786 info.go:266] docker info: {ID:2ddc239e-00f5-4d38-88ba-714604312793 Containers:37 ContainersRunning:32 ContainersPaused:0 ContainersStopped:5 Images:32 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:143 OomKillDisable:false NGoroutines:182 SystemTime:2025-09-23 20:03:12.008280597 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8266170368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.28.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.27] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.18.0] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-model] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.40] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0923 22:03:12.115156    3786 docker.go:318] overlay module found
I0923 22:03:12.196394    3786 out.go:179] ‚ú®  Using the docker driver based on existing profile
I0923 22:03:12.257059    3786 start.go:304] selected driver: docker
I0923 22:03:12.257080    3786 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 22:03:12.257296    3786 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0923 22:03:12.257602    3786 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0923 22:03:12.725113    3786 info.go:266] docker info: {ID:2ddc239e-00f5-4d38-88ba-714604312793 Containers:37 ContainersRunning:32 ContainersPaused:0 ContainersStopped:5 Images:32 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:142 OomKillDisable:false NGoroutines:175 SystemTime:2025-09-23 20:03:12.709739247 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8266170368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.28.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.27] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.18.0] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-model] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.40] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0923 22:03:12.725967    3786 cni.go:84] Creating CNI manager for ""
I0923 22:03:12.726026    3786 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0923 22:03:12.726082    3786 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 22:03:12.782051    3786 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0923 22:03:12.842549    3786 cache.go:123] Beginning downloading kic base image for docker with docker
I0923 22:03:12.908677    3786 out.go:179] üöú  Pulling base image v0.0.48 ...
I0923 22:03:12.976995    3786 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0923 22:03:12.977065    3786 preload.go:146] Found local preload: /home/adziallas/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0923 22:03:12.977073    3786 cache.go:58] Caching tarball of preloaded images
I0923 22:03:12.978097    3786 preload.go:172] Found /home/adziallas/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0923 22:03:12.978134    3786 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0923 22:03:12.978234    3786 profile.go:143] Saving config to /home/adziallas/.minikube/profiles/minikube/config.json ...
I0923 22:03:12.982517    3786 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0923 22:03:13.687471    3786 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I0923 22:03:13.688097    3786 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I0923 22:03:13.801058    3786 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I0923 22:03:13.801181    3786 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I0923 22:03:13.801229    3786 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I0923 22:03:13.801240    3786 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I0923 22:03:47.258294    3786 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I0923 22:03:47.258318    3786 cache.go:232] Successfully downloaded all kic artifacts
I0923 22:03:47.258359    3786 start.go:360] acquireMachinesLock for minikube: {Name:mk1ef3f1084e8f0d7084a1aec1fd58c8d33f8558 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0923 22:03:47.258702    3786 start.go:364] duration metric: took 319.95¬µs to acquireMachinesLock for "minikube"
I0923 22:03:47.258724    3786 start.go:96] Skipping create...Using existing machine configuration
I0923 22:03:47.258728    3786 fix.go:54] fixHost starting: 
I0923 22:03:47.259539    3786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 22:03:47.455811    3786 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0923 22:03:47.455827    3786 fix.go:138] unexpected machine state, will restart: <nil>
I0923 22:03:47.551089    3786 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I0923 22:03:47.551286    3786 cli_runner.go:164] Run: docker start minikube
I0923 22:03:53.338991    3786 cli_runner.go:217] Completed: docker start minikube: (5.78767851s)
I0923 22:03:53.339130    3786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 22:03:53.428577    3786 kic.go:430] container "minikube" state is running.
I0923 22:03:53.429737    3786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 22:03:53.526844    3786 profile.go:143] Saving config to /home/adziallas/.minikube/profiles/minikube/config.json ...
I0923 22:03:53.527655    3786 machine.go:93] provisionDockerMachine start ...
I0923 22:03:53.527742    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:03:53.685066    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:03:53.712851    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:03:53.712861    3786 main.go:141] libmachine: About to run SSH command:
hostname
I0923 22:03:53.792068    3786 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:34102->127.0.0.1:63224: read: connection reset by peer
I0923 22:04:00.049109    3786 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39504->127.0.0.1:63224: read: connection reset by peer
I0923 22:04:03.682741    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0923 22:04:03.682755    3786 ubuntu.go:182] provisioning hostname "minikube"
I0923 22:04:03.682835    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:03.730028    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:04:03.730521    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:04:03.730542    3786 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0923 22:04:04.524142    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0923 22:04:04.524219    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:04.605440    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:04:04.606146    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:04:04.606183    3786 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0923 22:04:04.837660    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0923 22:04:04.837673    3786 ubuntu.go:188] set auth options {CertDir:/home/adziallas/.minikube CaCertPath:/home/adziallas/.minikube/certs/ca.pem CaPrivateKeyPath:/home/adziallas/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/adziallas/.minikube/machines/server.pem ServerKeyPath:/home/adziallas/.minikube/machines/server-key.pem ClientKeyPath:/home/adziallas/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/adziallas/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/adziallas/.minikube}
I0923 22:04:04.837691    3786 ubuntu.go:190] setting up certificates
I0923 22:04:04.837704    3786 provision.go:84] configureAuth start
I0923 22:04:04.837756    3786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 22:04:04.942412    3786 provision.go:143] copyHostCerts
I0923 22:04:05.006542    3786 exec_runner.go:144] found /home/adziallas/.minikube/ca.pem, removing ...
I0923 22:04:05.006601    3786 exec_runner.go:203] rm: /home/adziallas/.minikube/ca.pem
I0923 22:04:05.052925    3786 exec_runner.go:151] cp: /home/adziallas/.minikube/certs/ca.pem --> /home/adziallas/.minikube/ca.pem (1086 bytes)
I0923 22:04:05.078017    3786 exec_runner.go:144] found /home/adziallas/.minikube/cert.pem, removing ...
I0923 22:04:05.078056    3786 exec_runner.go:203] rm: /home/adziallas/.minikube/cert.pem
I0923 22:04:05.078266    3786 exec_runner.go:151] cp: /home/adziallas/.minikube/certs/cert.pem --> /home/adziallas/.minikube/cert.pem (1131 bytes)
I0923 22:04:05.095154    3786 exec_runner.go:144] found /home/adziallas/.minikube/key.pem, removing ...
I0923 22:04:05.095167    3786 exec_runner.go:203] rm: /home/adziallas/.minikube/key.pem
I0923 22:04:05.095244    3786 exec_runner.go:151] cp: /home/adziallas/.minikube/certs/key.pem --> /home/adziallas/.minikube/key.pem (1679 bytes)
I0923 22:04:05.105100    3786 provision.go:117] generating server cert: /home/adziallas/.minikube/machines/server.pem ca-key=/home/adziallas/.minikube/certs/ca.pem private-key=/home/adziallas/.minikube/certs/ca-key.pem org=adziallas.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0923 22:04:05.706719    3786 provision.go:177] copyRemoteCerts
I0923 22:04:05.706891    3786 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0923 22:04:05.706987    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:05.782208    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:05.964728    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0923 22:04:06.288663    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0923 22:04:06.373104    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0923 22:04:06.482359    3786 provision.go:87] duration metric: took 1.64463129s to configureAuth
I0923 22:04:06.482391    3786 ubuntu.go:206] setting minikube options for container-runtime
I0923 22:04:06.482834    3786 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0923 22:04:06.482958    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:06.588605    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:04:06.588787    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:04:06.588793    3786 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0923 22:04:07.039684    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0923 22:04:07.039695    3786 ubuntu.go:71] root file system type: overlay
I0923 22:04:07.039849    3786 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0923 22:04:07.040007    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:07.127053    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:04:07.127314    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:04:07.127405    3786 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0923 22:04:07.308999    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0923 22:04:07.309060    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:07.345644    3786 main.go:141] libmachine: Using SSH client type: native
I0923 22:04:07.345853    3786 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 63224 <nil> <nil>}
I0923 22:04:07.345865    3786 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0923 22:04:07.729634    3786 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0923 22:04:07.729660    3786 machine.go:96] duration metric: took 10.980904962s to provisionDockerMachine
I0923 22:04:07.729703    3786 start.go:293] postStartSetup for "minikube" (driver="docker")
I0923 22:04:07.729725    3786 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0923 22:04:07.729853    3786 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0923 22:04:07.729955    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:07.831266    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:07.985894    3786 ssh_runner.go:195] Run: cat /etc/os-release
I0923 22:04:07.994781    3786 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0923 22:04:07.994797    3786 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0923 22:04:07.994802    3786 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0923 22:04:07.994806    3786 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0923 22:04:07.994814    3786 filesync.go:126] Scanning /home/adziallas/.minikube/addons for local assets ...
I0923 22:04:08.014338    3786 filesync.go:126] Scanning /home/adziallas/.minikube/files for local assets ...
I0923 22:04:08.083061    3786 start.go:296] duration metric: took 353.32695ms for postStartSetup
I0923 22:04:08.083374    3786 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0923 22:04:08.083474    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:08.218430    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:08.514079    3786 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0923 22:04:08.533931    3786 fix.go:56] duration metric: took 18.054110082s for fixHost
I0923 22:04:08.533944    3786 start.go:83] releasing machines lock for "minikube", held for 18.054147342s
I0923 22:04:08.534009    3786 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0923 22:04:08.605018    3786 ssh_runner.go:195] Run: cat /version.json
I0923 22:04:08.605077    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:08.605509    3786 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0923 22:04:08.605576    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:08.648956    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:08.752752    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:08.799542    3786 ssh_runner.go:195] Run: systemctl --version
I0923 22:04:09.018338    3786 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0923 22:04:10.617157    3786 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.01161718s)
I0923 22:04:10.617269    3786 ssh_runner.go:235] Completed: sh -c "stat /etc/cni/net.d/*loopback.conf*": (1.59891453s)
I0923 22:04:10.617327    3786 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0923 22:04:10.661300    3786 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0923 22:04:10.661353    3786 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0923 22:04:10.675623    3786 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0923 22:04:10.675647    3786 start.go:495] detecting cgroup driver to use...
I0923 22:04:10.675697    3786 detect.go:190] detected "systemd" cgroup driver on host os
I0923 22:04:10.700820    3786 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0923 22:04:10.731649    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0923 22:04:10.852225    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0923 22:04:10.874986    3786 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0923 22:04:10.875035    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0923 22:04:10.893086    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0923 22:04:10.909976    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0923 22:04:10.929690    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0923 22:04:10.965127    3786 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0923 22:04:10.982995    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0923 22:04:11.006517    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0923 22:04:11.040801    3786 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0923 22:04:11.074287    3786 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0923 22:04:11.113632    3786 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0923 22:04:11.179406    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:11.361065    3786 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0923 22:04:11.497012    3786 start.go:495] detecting cgroup driver to use...
I0923 22:04:11.497059    3786 detect.go:190] detected "systemd" cgroup driver on host os
I0923 22:04:11.497101    3786 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0923 22:04:11.531747    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0923 22:04:11.564259    3786 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0923 22:04:11.613013    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0923 22:04:11.632523    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0923 22:04:11.676684    3786 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0923 22:04:11.703098    3786 ssh_runner.go:195] Run: which cri-dockerd
I0923 22:04:11.709232    3786 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0923 22:04:11.725729    3786 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0923 22:04:11.753651    3786 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0923 22:04:12.074317    3786 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0923 22:04:12.212660    3786 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I0923 22:04:12.212737    3786 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0923 22:04:12.257524    3786 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0923 22:04:12.306492    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:12.447334    3786 ssh_runner.go:195] Run: sudo systemctl restart docker
I0923 22:04:26.076958    3786 ssh_runner.go:235] Completed: sudo systemctl restart docker: (13.629603187s)
I0923 22:04:26.077046    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0923 22:04:26.113260    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0923 22:04:26.166487    3786 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0923 22:04:26.248163    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0923 22:04:26.279074    3786 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0923 22:04:26.413245    3786 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0923 22:04:26.546540    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:26.651768    3786 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0923 22:04:26.709559    3786 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0923 22:04:26.767549    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:26.893732    3786 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0923 22:04:33.634242    3786 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (3.411140451s)
I0923 22:04:33.634352    3786 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0923 22:04:33.668935    3786 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0923 22:04:33.669158    3786 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0923 22:04:33.676167    3786 start.go:563] Will wait 60s for crictl version
I0923 22:04:33.676217    3786 ssh_runner.go:195] Run: which crictl
I0923 22:04:33.682724    3786 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0923 22:04:34.689208    3786 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.00646595s)
I0923 22:04:34.689220    3786 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0923 22:04:34.689264    3786 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0923 22:04:36.228839    3786 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (1.53955935s)
I0923 22:04:36.228885    3786 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0923 22:04:36.368836    3786 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I0923 22:04:36.369344    3786 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0923 22:04:36.445270    3786 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0923 22:04:36.452204    3786 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0923 22:04:36.471444    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0923 22:04:36.509594    3786 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0923 22:04:36.509679    3786 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0923 22:04:36.509771    3786 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0923 22:04:36.540832    3786 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0923 22:04:36.540846    3786 docker.go:621] Images already preloaded, skipping extraction
I0923 22:04:36.540895    3786 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0923 22:04:36.581601    3786 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0923 22:04:36.581611    3786 cache_images.go:85] Images are preloaded, skipping loading
I0923 22:04:36.581617    3786 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.34.0 docker true true} ...
I0923 22:04:36.581766    3786 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0923 22:04:36.581838    3786 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0923 22:04:40.654406    3786 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (4.072543434s)
I0923 22:04:40.654459    3786 cni.go:84] Creating CNI manager for ""
I0923 22:04:40.654482    3786 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0923 22:04:40.654540    3786 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0923 22:04:40.654573    3786 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0923 22:04:40.654780    3786 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0923 22:04:40.654944    3786 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0923 22:04:40.710082    3786 binaries.go:44] Found k8s binaries, skipping transfer
I0923 22:04:40.710213    3786 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0923 22:04:40.744392    3786 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0923 22:04:40.789895    3786 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0923 22:04:40.836555    3786 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0923 22:04:40.896173    3786 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0923 22:04:40.909263    3786 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0923 22:04:40.953645    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:41.119028    3786 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0923 22:04:41.164877    3786 certs.go:68] Setting up /home/adziallas/.minikube/profiles/minikube for IP: 192.168.58.2
I0923 22:04:41.164904    3786 certs.go:194] generating shared ca certs ...
I0923 22:04:41.164972    3786 certs.go:226] acquiring lock for ca certs: {Name:mk7e707b579105ac48d08cbf48bf5a9316b0f8f5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 22:04:41.167192    3786 certs.go:235] skipping valid "minikubeCA" ca cert: /home/adziallas/.minikube/ca.key
I0923 22:04:41.201278    3786 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/adziallas/.minikube/proxy-client-ca.key
I0923 22:04:41.201306    3786 certs.go:256] generating profile certs ...
I0923 22:04:41.201479    3786 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/adziallas/.minikube/profiles/minikube/client.key
I0923 22:04:41.219667    3786 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/adziallas/.minikube/profiles/minikube/apiserver.key.502bbb95
I0923 22:04:41.234113    3786 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/adziallas/.minikube/profiles/minikube/proxy-client.key
I0923 22:04:41.234526    3786 certs.go:484] found cert: /home/adziallas/.minikube/certs/ca-key.pem (1675 bytes)
I0923 22:04:41.234596    3786 certs.go:484] found cert: /home/adziallas/.minikube/certs/ca.pem (1086 bytes)
I0923 22:04:41.234647    3786 certs.go:484] found cert: /home/adziallas/.minikube/certs/cert.pem (1131 bytes)
I0923 22:04:41.234696    3786 certs.go:484] found cert: /home/adziallas/.minikube/certs/key.pem (1679 bytes)
I0923 22:04:41.235749    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0923 22:04:41.493812    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0923 22:04:41.613917    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0923 22:04:41.695733    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0923 22:04:41.732038    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0923 22:04:41.772884    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0923 22:04:41.810817    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0923 22:04:41.850475    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0923 22:04:41.897364    3786 ssh_runner.go:362] scp /home/adziallas/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0923 22:04:41.952322    3786 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0923 22:04:42.000692    3786 ssh_runner.go:195] Run: openssl version
I0923 22:04:42.088290    3786 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0923 22:04:42.169892    3786 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0923 22:04:42.188069    3786 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 14 17:53 /usr/share/ca-certificates/minikubeCA.pem
I0923 22:04:42.188161    3786 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0923 22:04:42.215261    3786 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0923 22:04:42.243082    3786 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0923 22:04:42.251745    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0923 22:04:42.363918    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0923 22:04:42.440194    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0923 22:04:42.478605    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0923 22:04:42.505722    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0923 22:04:42.519414    3786 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0923 22:04:42.539613    3786 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0923 22:04:42.539955    3786 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0923 22:04:42.683543    3786 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0923 22:04:42.702672    3786 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0923 22:04:42.702692    3786 kubeadm.go:589] restartPrimaryControlPlane start ...
I0923 22:04:42.702743    3786 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0923 22:04:42.725821    3786 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0923 22:04:42.725874    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0923 22:04:42.787638    3786 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:60415"
I0923 22:04:42.787659    3786 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:60415, want: 127.0.0.1:63228
I0923 22:04:42.788099    3786 kubeconfig.go:62] /home/adziallas/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0923 22:04:42.788635    3786 lock.go:35] WriteFile acquiring /home/adziallas/.kube/config: {Name:mkbe17ad6276561fac2c735d3c132d1566c8a9fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 22:04:43.539617    3786 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0923 22:04:43.603715    3786 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0923 22:04:43.603732    3786 kubeadm.go:593] duration metric: took 901.03752ms to restartPrimaryControlPlane
I0923 22:04:43.603739    3786 kubeadm.go:394] duration metric: took 1.06414884s to StartCluster
I0923 22:04:43.603752    3786 settings.go:142] acquiring lock: {Name:mkff0d2ddf8a05934ae076dd91d39674dbffff2b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 22:04:43.603878    3786 settings.go:150] Updating kubeconfig:  /home/adziallas/.kube/config
I0923 22:04:43.604761    3786 lock.go:35] WriteFile acquiring /home/adziallas/.kube/config: {Name:mkbe17ad6276561fac2c735d3c132d1566c8a9fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0923 22:04:43.604978    3786 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0923 22:04:43.605065    3786 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0923 22:04:43.605170    3786 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0923 22:04:43.605180    3786 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0923 22:04:43.605185    3786 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0923 22:04:43.605189    3786 addons.go:247] addon storage-provisioner should already be in state true
I0923 22:04:43.605207    3786 host.go:66] Checking if "minikube" exists ...
I0923 22:04:43.605231    3786 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0923 22:04:43.605239    3786 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0923 22:04:43.605454    3786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 22:04:43.605551    3786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 22:04:43.684760    3786 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0923 22:04:43.684775    3786 addons.go:247] addon default-storageclass should already be in state true
I0923 22:04:43.684793    3786 host.go:66] Checking if "minikube" exists ...
I0923 22:04:43.685147    3786 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0923 22:04:43.734925    3786 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0923 22:04:43.734935    3786 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0923 22:04:43.734985    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:43.780898    3786 out.go:179] üîé  Verifying Kubernetes components...
I0923 22:04:43.793950    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:43.902227    3786 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0923 22:04:43.902362    3786 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0923 22:04:43.993677    3786 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:04:43.993688    3786 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0923 22:04:43.993739    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0923 22:04:44.026889    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0923 22:04:44.064707    3786 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63224 SSHKeyPath:/home/adziallas/.minikube/machines/minikube/id_rsa Username:docker}
I0923 22:04:44.106012    3786 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0923 22:04:44.242853    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:04:53.748293    3786 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (9.721380439s)
W0923 22:04:53.748323    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:53.748345    3786 retry.go:31] will retry after 297.064309ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:53.748406    3786 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (9.642376729s)
I0923 22:04:53.748514    3786 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0923 22:04:53.755528    3786 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.512655409s)
W0923 22:04:53.755550    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:53.755562    3786 retry.go:31] will retry after 243.666654ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:53.801149    3786 api_server.go:52] waiting for apiserver process to appear ...
I0923 22:04:53.801204    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:53.999967    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:04:54.046582    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:04:54.220331    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:54.220347    3786 retry.go:31] will retry after 200.00707ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0923 22:04:54.247689    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:54.247704    3786 retry.go:31] will retry after 519.472039ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:54.302080    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:54.421434    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0923 22:04:54.522843    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:54.522880    3786 retry.go:31] will retry after 571.161885ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:54.768415    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0923 22:04:54.801731    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0923 22:04:55.006253    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:55.006267    3786 retry.go:31] will retry after 816.984428ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:55.094782    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0923 22:04:55.271054    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:55.271087    3786 retry.go:31] will retry after 881.282786ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:55.301310    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:55.801864    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:55.823827    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:04:55.892505    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:55.892519    3786 retry.go:31] will retry after 826.368943ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:56.153181    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0923 22:04:56.235604    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:56.235618    3786 retry.go:31] will retry after 1.496728949s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:56.302147    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:56.719297    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:04:56.789484    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:56.789499    3786 retry.go:31] will retry after 1.755998114s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:56.801894    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:57.301956    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:57.733424    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:04:57.801499    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0923 22:04:57.817142    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:57.817157    3786 retry.go:31] will retry after 1.395420091s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:58.302133    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:58.545713    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:04:58.647013    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:58.647032    3786 retry.go:31] will retry after 2.701652065s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:58.802578    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:04:59.213423    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:04:59.301338    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0923 22:04:59.368928    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:59.368948    3786 retry.go:31] will retry after 1.941526139s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:04:59.802777    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:00.303010    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:00.801503    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:01.303587    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:01.310613    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:05:01.349386    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:05:01.737803    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:01.737819    3786 retry.go:31] will retry after 5.418707727s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:01.813484    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0923 22:05:01.918703    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:01.918722    3786 retry.go:31] will retry after 2.738914146s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:02.305936    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:02.801772    3786 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0923 22:05:02.847906    3786 api_server.go:72] duration metric: took 19.242904036s to wait for apiserver process to appear ...
I0923 22:05:02.847927    3786 api_server.go:88] waiting for apiserver healthz status ...
I0923 22:05:02.847940    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:02.875840    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60148->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:06.684910    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:06.685838    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:07.185308    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:07.187759    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:07.685476    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:07.686646    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60158->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:07.995310    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:05:08.085186    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:08.085214    3786 retry.go:31] will retry after 3.139102469s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:08.185580    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:08.187181    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:08.685702    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:08.686889    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60174->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:09.184729    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:09.186187    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:09.684731    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:09.685570    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60200->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:10.185164    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:10.187180    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:10.493544    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0923 22:05:10.587299    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:10.587344    3786 retry.go:31] will retry after 5.364722819s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:10.685699    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:10.686660    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60220->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:11.184778    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:11.185503    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60224->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:11.225474    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:05:11.389742    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:11.389756    3786 retry.go:31] will retry after 3.548416809s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:11.686176    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:11.689967    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60236->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:12.184806    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:12.187494    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:12.685048    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:12.686449    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60256->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:13.185262    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:13.187384    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:60270->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:13.685164    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:13.686255    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39640->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:14.185155    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:14.186118    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39642->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:14.685579    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:14.686532    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39650->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:14.939047    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0923 22:05:15.068524    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:15.068542    3786 retry.go:31] will retry after 14.373841402s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:15.184795    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:15.186949    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39664->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:15.684923    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:15.688038    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:15.952528    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:05:16.184660    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:16.186499    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39682->127.0.0.1:63228: read: connection reset by peer
W0923 22:05:16.282387    3786 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:16.282402    3786 retry.go:31] will retry after 8.298949871s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0923 22:05:16.685003    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:16.687569    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": read tcp 127.0.0.1:39688->127.0.0.1:63228: read: connection reset by peer
I0923 22:05:17.185345    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:17.186160    3786 api_server.go:269] stopped: https://127.0.0.1:63228/healthz: Get "https://127.0.0.1:63228/healthz": EOF
I0923 22:05:17.685446    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:19.642282    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0923 22:05:19.642295    3786 api_server.go:103] status: https://127.0.0.1:63228/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0923 22:05:19.642305    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:19.747009    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0923 22:05:19.747045    3786 api_server.go:103] status: https://127.0.0.1:63228/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0923 22:05:19.747057    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:19.810211    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0923 22:05:19.810224    3786 api_server.go:103] status: https://127.0.0.1:63228/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0923 22:05:20.185717    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:20.231943    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0923 22:05:20.232051    3786 api_server.go:103] status: https://127.0.0.1:63228/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0923 22:05:20.685764    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:20.700315    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0923 22:05:20.700340    3786 api_server.go:103] status: https://127.0.0.1:63228/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0923 22:05:21.185913    3786 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63228/healthz ...
I0923 22:05:21.203731    3786 api_server.go:279] https://127.0.0.1:63228/healthz returned 200:
ok
I0923 22:05:21.236167    3786 api_server.go:141] control plane version: v1.34.0
I0923 22:05:21.236185    3786 api_server.go:131] duration metric: took 15.051601839s to wait for apiserver health ...
I0923 22:05:21.236194    3786 system_pods.go:43] waiting for kube-system pods to appear ...
I0923 22:05:21.383509    3786 system_pods.go:59] 7 kube-system pods found
I0923 22:05:21.383576    3786 system_pods.go:61] "coredns-66bc5c9577-djfsq" [e3794b62-ff17-44e3-a0d9-9c5839df2186] Running
I0923 22:05:21.383587    3786 system_pods.go:61] "etcd-minikube" [74797e5d-98a2-4076-91ff-7459f6590a82] Running
I0923 22:05:21.383596    3786 system_pods.go:61] "kube-apiserver-minikube" [ae17c3c5-80ec-4ae2-be77-aef34bcd5aaa] Running
I0923 22:05:21.383604    3786 system_pods.go:61] "kube-controller-manager-minikube" [900a1f98-0449-4d8e-b94d-268d365affbc] Running
I0923 22:05:21.383613    3786 system_pods.go:61] "kube-proxy-qxgs4" [6d250f50-8e87-4753-9216-4a6d406cc117] Running
I0923 22:05:21.383621    3786 system_pods.go:61] "kube-scheduler-minikube" [b643d5a4-9fac-4c5e-8544-dac0976b5a62] Running
I0923 22:05:21.383627    3786 system_pods.go:61] "storage-provisioner" [540494df-e37a-478b-9008-1679d4a1a13d] Running
I0923 22:05:21.383639    3786 system_pods.go:74] duration metric: took 147.43512ms to wait for pod list to return data ...
I0923 22:05:21.383662    3786 kubeadm.go:578] duration metric: took 34.442002915s to wait for: map[apiserver:true system_pods:true]
I0923 22:05:21.383687    3786 node_conditions.go:102] verifying NodePressure condition ...
I0923 22:05:21.393625    3786 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0923 22:05:21.393751    3786 node_conditions.go:123] node cpu capacity is 4
I0923 22:05:21.393778    3786 node_conditions.go:105] duration metric: took 10.08234ms to run NodePressure ...
I0923 22:05:21.393805    3786 start.go:241] waiting for startup goroutines ...
I0923 22:05:24.581917    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0923 22:05:25.777082    3786 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.19513835s)
I0923 22:05:29.443606    3786 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0923 22:05:29.990089    3786 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I0923 22:05:30.118033    3786 addons.go:514] duration metric: took 43.17626819s for enable addons: enabled=[storage-provisioner default-storageclass]
I0923 22:05:30.118078    3786 start.go:246] waiting for cluster config update ...
I0923 22:05:30.118089    3786 start.go:255] writing updated cluster config ...
I0923 22:05:30.118604    3786 ssh_runner.go:195] Run: rm -f paused
I0923 22:05:30.383316    3786 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I0923 22:05:30.622055    3786 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 28 01:18:18 minikube cri-dockerd[882]: time="2025-09-28T01:18:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/935f8f44cbbc1ffbee4010e249cc2167c170a61edf12c69171628fbad862a106/resolv.conf as [nameserver 10.96.0.10 search prod.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 28 01:18:20 minikube cri-dockerd[882]: time="2025-09-28T01:18:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44395d152bb7365d9e63ba10ad1b02a2c3d96fa79e2b8705366029ebea6a2761/resolv.conf as [nameserver 10.96.0.10 search prod.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 28 01:18:51 minikube cri-dockerd[882]: time="2025-09-28T01:18:51Z" level=info msg="Stop pulling image docker.io/andziallas/kukuk-frontend:latest: Status: Downloaded newer image for andziallas/kukuk-frontend:latest"
Sep 28 01:19:07 minikube cri-dockerd[882]: time="2025-09-28T01:19:07Z" level=info msg="Pulling image docker.io/andziallas/backend-kukuk:latest: 48c0536b3dd9: Pulling fs layer "
Sep 28 01:19:17 minikube cri-dockerd[882]: time="2025-09-28T01:19:17Z" level=info msg="Pulling image docker.io/andziallas/backend-kukuk:latest: 48c0536b3dd9: Downloading [=============================================>     ]  15.61MB/17.13MB"
Sep 28 01:19:27 minikube cri-dockerd[882]: time="2025-09-28T01:19:27Z" level=info msg="Pulling image docker.io/andziallas/backend-kukuk:latest: 48c0536b3dd9: Pull complete "
Sep 28 01:19:30 minikube cri-dockerd[882]: time="2025-09-28T01:19:30Z" level=info msg="Stop pulling image docker.io/andziallas/backend-kukuk:latest: Status: Downloaded newer image for andziallas/backend-kukuk:latest"
Sep 28 01:19:38 minikube cri-dockerd[882]: time="2025-09-28T01:19:38Z" level=info msg="Stop pulling image docker.io/andziallas/backend-kukuk:latest: Status: Image is up to date for andziallas/backend-kukuk:latest"
Sep 28 01:19:43 minikube cri-dockerd[882]: time="2025-09-28T01:19:43Z" level=info msg="Stop pulling image docker.io/andziallas/kukuk-frontend:latest: Status: Image is up to date for andziallas/kukuk-frontend:latest"
Sep 28 01:42:18 minikube cri-dockerd[882]: time="2025-09-28T01:42:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dc7c10d8d93931915d95ff1346476ff4c2c11d92df45a2e825042e005fc734cf/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 28 01:42:19 minikube cri-dockerd[882]: time="2025-09-28T01:42:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af38af41750d9e867622a511cd50c4d58512c4ac1e366290761d7ba5fd8ba4bf/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 28 01:42:22 minikube dockerd[552]: time="2025-09-28T01:42:22.196201279Z" level=warning msg="reference for unknown type: " digest="sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Sep 28 01:42:36 minikube cri-dockerd[882]: time="2025-09-28T01:42:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Download complete "
Sep 28 01:42:46 minikube cri-dockerd[882]: time="2025-09-28T01:42:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 045fc1c20da8: Extracting [==================================================>]      93B/93B"
Sep 28 01:42:56 minikube cri-dockerd[882]: time="2025-09-28T01:42:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: ddf74a63f7d8: Extracting [==================================================>]  131.9kB/131.9kB"
Sep 28 01:43:08 minikube cri-dockerd[882]: time="2025-09-28T01:43:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [==================>                                ]  10.32MB/27.61MB"
Sep 28 01:43:19 minikube cri-dockerd[882]: time="2025-09-28T01:43:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [=========================>                         ]  13.86MB/27.61MB"
Sep 28 01:43:28 minikube cri-dockerd[882]: time="2025-09-28T01:43:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [====================================>              ]  20.05MB/27.61MB"
Sep 28 01:43:40 minikube cri-dockerd[882]: time="2025-09-28T01:43:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [=========================================>         ]  22.71MB/27.61MB"
Sep 28 01:43:51 minikube cri-dockerd[882]: time="2025-09-28T01:43:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [============================================>      ]  24.77MB/27.61MB"
Sep 28 01:44:00 minikube cri-dockerd[882]: time="2025-09-28T01:44:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Extracting [=================================================> ]  27.13MB/27.61MB"
Sep 28 01:44:04 minikube cri-dockerd[882]: time="2025-09-28T01:44:04Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Sep 28 01:44:13 minikube cri-dockerd[882]: time="2025-09-28T01:44:13Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Sep 28 01:44:20 minikube dockerd[552]: time="2025-09-28T01:44:20.482175891Z" level=info msg="ignoring event" container=29b4c1c043145a91bff22cecc2f355e7869105b35842cdf8b1d891b62883cc59 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 28 01:44:25 minikube dockerd[552]: time="2025-09-28T01:44:25.143867906Z" level=info msg="ignoring event" container=51798881ca2b6fe7bb294cd64c77ebd6e966542d581c7164e6d3f7acb5f8a792 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 28 01:44:37 minikube dockerd[552]: time="2025-09-28T01:44:37.758422651Z" level=info msg="ignoring event" container=321e936186fc5ee9bda677617e810804b2a1709f5e1d0097ebb03aab49cbf603 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 28 01:44:39 minikube dockerd[552]: time="2025-09-28T01:44:39.207844590Z" level=info msg="ignoring event" container=dc7c10d8d93931915d95ff1346476ff4c2c11d92df45a2e825042e005fc734cf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 28 01:44:44 minikube dockerd[552]: time="2025-09-28T01:44:44.590296096Z" level=info msg="ignoring event" container=af38af41750d9e867622a511cd50c4d58512c4ac1e366290761d7ba5fd8ba4bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 28 01:46:40 minikube cri-dockerd[882]: time="2025-09-28T01:46:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/91652f68a1ec32af679611a6a9e0870f3838b0242c8b6d89e387399f0f560db5/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 28 01:46:53 minikube dockerd[552]: time="2025-09-28T01:46:53.945702821Z" level=warning msg="reference for unknown type: " digest="sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef" remote="registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef"
Sep 28 01:47:08 minikube cri-dockerd[882]: time="2025-09-28T01:47:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: c143cf4725f8: Downloading [========================================>          ]  3.951MB/4.82MB"
Sep 28 01:47:19 minikube cri-dockerd[882]: time="2025-09-28T01:47:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Downloading [>                                                  ]  261.3kB/25.21MB"
Sep 28 01:47:29 minikube cri-dockerd[882]: time="2025-09-28T01:47:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [===========================================>       ]  29.25MB/33.68MB"
Sep 28 01:47:39 minikube cri-dockerd[882]: time="2025-09-28T01:47:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [===============>                                   ]  6.789MB/22.11MB"
Sep 28 01:47:49 minikube cri-dockerd[882]: time="2025-09-28T01:47:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [============================================>      ]  19.63MB/22.11MB"
Sep 28 01:48:01 minikube cri-dockerd[882]: time="2025-09-28T01:48:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Extracting [=============>                                     ]  9.372MB/33.68MB"
Sep 28 01:48:11 minikube cri-dockerd[882]: time="2025-09-28T01:48:11Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Extracting [=============================================>     ]  30.64MB/33.68MB"
Sep 28 01:48:21 minikube cri-dockerd[882]: time="2025-09-28T01:48:21Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Extracting [==============================================>    ]     31MB/33.68MB"
Sep 28 01:48:33 minikube cri-dockerd[882]: time="2025-09-28T01:48:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Extracting [===============================================>   ]  31.72MB/33.68MB"
Sep 28 01:48:43 minikube cri-dockerd[882]: time="2025-09-28T01:48:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: b25bd6379dd4: Extracting [>                                                  ]  65.54kB/5.153MB"
Sep 28 01:48:53 minikube cri-dockerd[882]: time="2025-09-28T01:48:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: c143cf4725f8: Extracting [>                                                  ]  65.54kB/4.82MB"
Sep 28 01:49:05 minikube cri-dockerd[882]: time="2025-09-28T01:49:05Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: c143cf4725f8: Pull complete "
Sep 28 01:49:15 minikube cri-dockerd[882]: time="2025-09-28T01:49:15Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [==>                                                ]  1.311MB/25.21MB"
Sep 28 01:49:25 minikube cri-dockerd[882]: time="2025-09-28T01:49:25Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [=======>                                           ]  3.932MB/25.21MB"
Sep 28 01:49:37 minikube cri-dockerd[882]: time="2025-09-28T01:49:37Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [===============>                                   ]  7.864MB/25.21MB"
Sep 28 01:49:47 minikube cri-dockerd[882]: time="2025-09-28T01:49:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [========================>                          ]  12.58MB/25.21MB"
Sep 28 01:49:57 minikube cri-dockerd[882]: time="2025-09-28T01:49:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [===================================>               ]  17.83MB/25.21MB"
Sep 28 01:50:09 minikube cri-dockerd[882]: time="2025-09-28T01:50:09Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Extracting [====================================>              ]  18.61MB/25.21MB"
Sep 28 01:50:19 minikube cri-dockerd[882]: time="2025-09-28T01:50:19Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 4f4fb700ef54: Extracting [==================================================>]      32B/32B"
Sep 28 01:50:29 minikube cri-dockerd[882]: time="2025-09-28T01:50:29Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d29fa5b0e5e2: Extracting [==================================================>]   56.8kB/56.8kB"
Sep 28 01:50:41 minikube cri-dockerd[882]: time="2025-09-28T01:50:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 00d7fdc125fd: Extracting [=========================================>         ]  2.294MB/2.75MB"
Sep 28 01:50:51 minikube cri-dockerd[882]: time="2025-09-28T01:50:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [=====>                                             ]  1.802MB/15.77MB"
Sep 28 01:51:01 minikube cri-dockerd[882]: time="2025-09-28T01:51:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [================>                                  ]  5.079MB/15.77MB"
Sep 28 01:51:13 minikube cri-dockerd[882]: time="2025-09-28T01:51:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [=================>                                 ]  5.571MB/15.77MB"
Sep 28 01:51:23 minikube cri-dockerd[882]: time="2025-09-28T01:51:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [===============================>                   ]  9.994MB/15.77MB"
Sep 28 01:51:33 minikube cri-dockerd[882]: time="2025-09-28T01:51:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [==========================================>        ]  13.27MB/15.77MB"
Sep 28 01:51:45 minikube cri-dockerd[882]: time="2025-09-28T01:51:45Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [==============================================>    ]  14.58MB/15.77MB"
Sep 28 01:51:58 minikube cri-dockerd[882]: time="2025-09-28T01:51:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [==============================================>    ]  14.58MB/15.77MB"
Sep 28 01:52:05 minikube cri-dockerd[882]: time="2025-09-28T01:52:05Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [===============================================>   ]  14.91MB/15.77MB"
Sep 28 01:52:17 minikube cri-dockerd[882]: time="2025-09-28T01:52:17Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9c0a1d8de43f: Extracting [==================================================>]  15.77MB/15.77MB"


==> container status <==
CONTAINER ID   IMAGE                                                COMMAND                  CREATED          STATUS                        PORTS     NAMES
91652f68a1ec   registry.k8s.io/pause:3.10.1                         "/pause"                 6 minutes ago    Up 6 minutes                            k8s_POD_ingress-nginx-controller-9cc49f96f-7ldqn_ingress-nginx_205a5c0d-a586-4910-8868-6754e002c4ac_0
321e936186fc   8c217da6734d                                         "/kube-webhook-certg‚Ä¶"   9 minutes ago    Exited (0) 8 minutes ago                k8s_patch_ingress-nginx-admission-patch-7vxbm_ingress-nginx_ae178390-ebda-4630-ac0a-f489da1e3250_1
51798881ca2b   registry.k8s.io/ingress-nginx/kube-webhook-certgen   "/kube-webhook-certg‚Ä¶"   9 minutes ago    Exited (0) 8 minutes ago                k8s_create_ingress-nginx-admission-create-csnq6_ingress-nginx_8082750c-d3f9-4265-8c10-9186d87c5d6c_0
af38af41750d   registry.k8s.io/pause:3.10.1                         "/pause"                 11 minutes ago   Exited (0) 8 minutes ago                k8s_POD_ingress-nginx-admission-patch-7vxbm_ingress-nginx_ae178390-ebda-4630-ac0a-f489da1e3250_0
dc7c10d8d939   registry.k8s.io/pause:3.10.1                         "/pause"                 11 minutes ago   Exited (0) 8 minutes ago                k8s_POD_ingress-nginx-admission-create-csnq6_ingress-nginx_8082750c-d3f9-4265-8c10-9186d87c5d6c_0
3c91a0f37dea   andziallas/kukuk-frontend                            "/docker-entrypoint.‚Ä¶"   33 minutes ago   Up 33 minutes                           k8s_frontend_frontend-6846b7747f-bfbbw_prod_11e4bf8e-b290-4985-8d5b-a427b3772b94_0
7c49754046cc   andziallas/backend-kukuk                             "java -jar app.jar"      33 minutes ago   Up 33 minutes                           k8s_backend_backend-6575c764b5-d9ddg_prod_a7fd7e1f-cf84-43ed-8ae5-dfcf43d74f2a_0
f4b2132244e2   andziallas/backend-kukuk                             "java -jar app.jar"      33 minutes ago   Up 33 minutes                           k8s_backend_backend-6575c764b5-m64x4_prod_f5123923-7a6c-45ab-8bf2-9193274184ff_0
516f845665bf   andziallas/kukuk-frontend                            "/docker-entrypoint.‚Ä¶"   34 minutes ago   Up 34 minutes                           k8s_frontend_frontend-6846b7747f-4ndkg_prod_3a48944c-7e66-45c4-a1d6-614c1b6142c6_0
44395d152bb7   registry.k8s.io/pause:3.10.1                         "/pause"                 35 minutes ago   Up 35 minutes                           k8s_POD_frontend-6846b7747f-bfbbw_prod_11e4bf8e-b290-4985-8d5b-a427b3772b94_0
ea16a9181879   registry.k8s.io/pause:3.10.1                         "/pause"                 35 minutes ago   Up 35 minutes                           k8s_POD_backend-6575c764b5-m64x4_prod_f5123923-7a6c-45ab-8bf2-9193274184ff_0
21a8d9640975   registry.k8s.io/pause:3.10.1                         "/pause"                 35 minutes ago   Up 35 minutes                           k8s_POD_frontend-6846b7747f-4ndkg_prod_3a48944c-7e66-45c4-a1d6-614c1b6142c6_0
935f8f44cbbc   registry.k8s.io/pause:3.10.1                         "/pause"                 35 minutes ago   Up 35 minutes                           k8s_POD_backend-6575c764b5-d9ddg_prod_a7fd7e1f-cf84-43ed-8ae5-dfcf43d74f2a_0
e43213248791   6e38f40d628d                                         "/storage-provisioner"   36 minutes ago   Exited (255) 15 seconds ago             k8s_storage-provisioner_storage-provisioner_kube-system_540494df-e37a-478b-9008-1679d4a1a13d_12
aba039da04cf   90550c43ad2b                                         "kube-apiserver --ad‚Ä¶"   36 hours ago     Up 36 hours                             k8s_kube-apiserver_kube-apiserver-minikube_kube-system_31ac333127b7ec8f75ecbfdf00bb85d4_3
2dde68910429   90550c43ad2b                                         "kube-apiserver --ad‚Ä¶"   37 hours ago     Exited (255) 36 hours ago               k8s_kube-apiserver_kube-apiserver-minikube_kube-system_31ac333127b7ec8f75ecbfdf00bb85d4_2
bbede24678a3   andziallas/backend-kukuk                             "java -jar app.jar"      4 days ago       Up 4 days                               k8s_backend_backend-7b6cd5cbc5-b2tgj_default_d07d1cc2-eb61-45ba-9a7e-50a96517551c_0
70cfcb469c05   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_backend-7b6cd5cbc5-b2tgj_default_d07d1cc2-eb61-45ba-9a7e-50a96517551c_0
ef4a69590665   andziallas/kukuk-frontend                            "/docker-entrypoint.‚Ä¶"   4 days ago       Up 4 days                               k8s_frontend_frontend-5d48899665-slk9d_default_66f91fdf-03c7-4cc8-9cc8-74d0ed594450_0
4d8270c940f5   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_frontend-5d48899665-slk9d_default_66f91fdf-03c7-4cc8-9cc8-74d0ed594450_0
401bd58a2c4a   52546a367cc9                                         "/coredns -conf /etc‚Ä¶"   4 days ago       Up 4 days                               k8s_coredns_coredns-66bc5c9577-djfsq_kube-system_e3794b62-ff17-44e3-a0d9-9c5839df2186_1
d8e33f985dab   df0860106674                                         "/usr/local/bin/kube‚Ä¶"   4 days ago       Up 4 days                               k8s_kube-proxy_kube-proxy-qxgs4_kube-system_6d250f50-8e87-4753-9216-4a6d406cc117_1
02f17bdbb8b9   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_coredns-66bc5c9577-djfsq_kube-system_e3794b62-ff17-44e3-a0d9-9c5839df2186_1
fdf89ca0d81b   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_kube-proxy-qxgs4_kube-system_6d250f50-8e87-4753-9216-4a6d406cc117_1
ef5cd3d79363   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_storage-provisioner_kube-system_540494df-e37a-478b-9008-1679d4a1a13d_1
8a643aa3bf49   5f1f5298c888                                         "etcd --advertise-cl‚Ä¶"   4 days ago       Up 4 days                               k8s_etcd_etcd-minikube_kube-system_2984930d46b1e7a87dd14a2b8d7597e0_1
f7e218163a7d   a0af72f2ec6d                                         "kube-controller-man‚Ä¶"   4 days ago       Up 4 days                               k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_3b51c8241e224d47681cce32ea99b407_3
60204e0b551b   46169d968e92                                         "kube-scheduler --au‚Ä¶"   4 days ago       Up 4 days                               k8s_kube-scheduler_kube-scheduler-minikube_kube-system_dc6cf0a7bcb54d1f95cecc4d7b6b7d67_1
18bdb1a659f3   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_kube-controller-manager-minikube_kube-system_3b51c8241e224d47681cce32ea99b407_1
0e08090aa382   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_kube-apiserver-minikube_kube-system_31ac333127b7ec8f75ecbfdf00bb85d4_1
62638cafb1ff   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_etcd-minikube_kube-system_2984930d46b1e7a87dd14a2b8d7597e0_1
28bdaed70122   registry.k8s.io/pause:3.10.1                         "/pause"                 4 days ago       Up 4 days                               k8s_POD_kube-scheduler-minikube_kube-system_dc6cf0a7bcb54d1f95cecc4d7b6b7d67_1
a5bd97d1de15   df0860106674                                         "/usr/local/bin/kube‚Ä¶"   10 days ago      Exited (255) 4 days ago                 k8s_kube-proxy_kube-proxy-qxgs4_kube-system_6d250f50-8e87-4753-9216-4a6d406cc117_0
bf235319924a   52546a367cc9                                         "/coredns -conf /etc‚Ä¶"   10 days ago      Exited (255) 4 days ago                 k8s_coredns_coredns-66bc5c9577-djfsq_kube-system_e3794b62-ff17-44e3-a0d9-9c5839df2186_0
38aad8f73cee   registry.k8s.io/pause:3.10.1                         "/pause"                 10 days ago      Exited (255) 4 days ago                 k8s_POD_kube-proxy-qxgs4_kube-system_6d250f50-8e87-4753-9216-4a6d406cc117_0
734a8cfc830f   registry.k8s.io/pause:3.10.1                         "/pause"                 10 days ago      Exited (255) 4 days ago                 k8s_POD_coredns-66bc5c9577-djfsq_kube-system_e3794b62-ff17-44e3-a0d9-9c5839df2186_0
171c42f73e29   a0af72f2ec6d                                         "kube-controller-man‚Ä¶"   10 days ago      Exited (255) 4 days ago                 k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_3b51c8241e224d47681cce32ea99b407_2
f24c63598434   5f1f5298c888                                         "etcd --advertise-cl‚Ä¶"   10 days ago      Exited (255) 4 days ago                 k8s_etcd_etcd-minikube_kube-system_2984930d46b1e7a87dd14a2b8d7597e0_0
45870de4c37d   46169d968e92                                         "kube-scheduler --au‚Ä¶"   10 days ago      Exited (255) 4 days ago                 k8s_kube-scheduler_kube-scheduler-minikube_kube-system_dc6cf0a7bcb54d1f95cecc4d7b6b7d67_0
0a6a3c0b7ed7   registry.k8s.io/pause:3.10.1                         "/pause"                 10 days ago      Exited (255) 4 days ago                 k8s_POD_etcd-minikube_kube-system_2984930d46b1e7a87dd14a2b8d7597e0_0
994a4eabdb03   registry.k8s.io/pause:3.10.1                         "/pause"                 10 days ago      Exited (255) 4 days ago                 k8s_POD_kube-scheduler-minikube_kube-system_dc6cf0a7bcb54d1f95cecc4d7b6b7d67_0
61972aad0f5b   registry.k8s.io/pause:3.10.1                         "/pause"                 10 days ago      Exited (255) 4 days ago                 k8s_POD_kube-controller-manager-minikube_kube-system_3b51c8241e224d47681cce32ea99b407_0
time="2025-09-28T01:53:22Z" level=fatal msg="unable to determine runtime API version: rpc error: code = DeadlineExceeded desc = context deadline exceeded"


==> coredns [401bd58a2c4a] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4440e7469c6be8c359d7b298db58524fed56dc1669734a56c9c4c387cf0e3c2da951c3e208d1d199d29de038fba78e514eb90088d28e87a2d2e4287d9ee777cc
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:35819 - 40610 "HINFO IN 4857903437263996448.6763576286481044964. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.05613003s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.360891997s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.590059128s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.121710971s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.0553075s


==> coredns [bf235319924a] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = 4440e7469c6be8c359d7b298db58524fed56dc1669734a56c9c4c387cf0e3c2da951c3e208d1d199d29de038fba78e514eb90088d28e87a2d2e4287d9ee777cc
[INFO] Reloading complete
[INFO] 127.0.0.1:40974 - 53096 "HINFO IN 4457693804193453874.1057856411809416099. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.06270455s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_17T05_26_05_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 17 Sep 2025 03:25:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Sep 2025 01:54:03 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Sep 2025 01:52:37 +0000   Fri, 26 Sep 2025 13:25:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Sep 2025 01:52:37 +0000   Fri, 26 Sep 2025 13:25:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Sep 2025 01:52:37 +0000   Fri, 26 Sep 2025 13:25:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Sep 2025 01:52:37 +0000   Fri, 26 Sep 2025 13:25:50 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8072432Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8072432Ki
  pods:               110
System Info:
  Machine ID:                 321464198a8842e8ad696967ded965ee
  System UUID:                321464198a8842e8ad696967ded965ee
  Boot ID:                    af48f1aa-bb69-40c5-87f1-666a48dced54
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     backend-7b6cd5cbc5-b2tgj                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d4h
  default                     frontend-5d48899665-slk9d                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d4h
  ingress-nginx               ingress-nginx-controller-9cc49f96f-7ldqn    100m (2%)     0 (0%)      90Mi (1%)        0 (0%)         12m
  kube-system                 coredns-66bc5c9577-djfsq                    100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     10d
  kube-system                 etcd-minikube                               100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         10d
  kube-system                 kube-apiserver-minikube                     250m (6%)     0 (0%)      0 (0%)           0 (0%)         10d
  kube-system                 kube-controller-manager-minikube            200m (5%)     0 (0%)      0 (0%)           0 (0%)         10d
  kube-system                 kube-proxy-qxgs4                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         10d
  kube-system                 kube-scheduler-minikube                     100m (2%)     0 (0%)      0 (0%)           0 (0%)         10d
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         10d
  prod                        backend-6575c764b5-d9ddg                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
  prod                        backend-6575c764b5-m64x4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
  prod                        frontend-6846b7747f-4ndkg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
  prod                        frontend-6846b7747f-bfbbw                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             260Mi (3%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Sep27 18:22] PCI: Fatal: No config space access function found
[  +0.039155] PCI: System does not support PCI
[  +0.125763] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.029021] hv_balloon: Cold memory discard hypercall failed with status 1900000005
[  +0.009042] hv_balloon: Underlying Hyper-V does not support order less than 9. Hypercall failed
[  +0.006640] hv_balloon: Defaulting to page_reporting_order 9
[  +3.438759] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#127 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +3.204889] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep27 18:23] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.044322] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +7.043408] pulseaudio[243]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[ +37.058991] Failed to connect to bus: No such file or directory
[  +0.261594] Failed to connect to bus: No such file or directory
[  +0.256615] Failed to connect to bus: No such file or directory
[  +0.256332] Failed to connect to bus: No such file or directory
[  +0.256301] Failed to connect to bus: No such file or directory
[  +0.266110] Failed to connect to bus: No such file or directory
[  +0.260733] Failed to connect to bus: No such file or directory
[  +0.257844] Failed to connect to bus: No such file or directory
[  +0.267732] Failed to connect to bus: No such file or directory
[  +0.264500] Failed to connect to bus: No such file or directory
[  +0.261462] Failed to connect to bus: No such file or directory
[  +0.261067] Failed to connect to bus: No such file or directory
[  +0.265712] Failed to connect to bus: No such file or directory
[  +0.687724] systemd-journald[85]: File /var/log/journal/3cd91cb81e044ccb9cbb7ca095cb9a11/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +4.989593] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3393: /sbin/init failed to start within 10000ms
[Sep27 18:24] WSL (2 - Interop) ERROR: CreateLoginSession:2732: Timed out waiting for user session for uid=1000
[ +27.578382] netlink: 'init': attribute type 4 has an invalid length.
[Sep27 18:54] hrtimer: interrupt took 2038680 ns
[Sep27 20:19] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +5.507214] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +5.453258] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep27 22:53] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +8.491871] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep28 00:19] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.067472] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[Sep28 01:04] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +8.954715] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep28 01:45] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +9.935236] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [8a643aa3bf49] <==
{"level":"warn","ts":"2025-09-28T01:54:18.560372Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"353.959429ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-09-28T01:54:18.562294Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"154.704196ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538348504950647 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller.18694f6c6ed3eae0\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller.18694f6c6ed3eae0\" value_size:642 lease:3238538348504950598 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-09-28T01:54:18.562402Z","caller":"traceutil/trace.go:172","msg":"trace[366296701] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13343; }","duration":"355.976242ms","start":"2025-09-28T01:54:18.206380Z","end":"2025-09-28T01:54:18.562356Z","steps":["trace[366296701] 'agreement among raft nodes before linearized reading'  (duration: 200.981483ms)","trace[366296701] 'range keys from in-memory index tree'  (duration: 152.848754ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:18.562561Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:18.206368Z","time spent":"356.164682ms","remote":"127.0.0.1:48374","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-09-28T01:54:18.562669Z","caller":"traceutil/trace.go:172","msg":"trace[1228980604] transaction","detail":"{read_only:false; response_revision:13345; number_of_response:1; }","duration":"687.325483ms","start":"2025-09-28T01:54:16.609613Z","end":"2025-09-28T01:54:18.562650Z","steps":["trace[1228980604] 'process raft request'  (duration: 687.179444ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:18.562802Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:16.609596Z","time spent":"687.432716ms","remote":"127.0.0.1:48662","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13334 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-09-28T01:54:18.563993Z","caller":"traceutil/trace.go:172","msg":"trace[934563775] transaction","detail":"{read_only:false; response_revision:13344; number_of_response:1; }","duration":"696.324335ms","start":"2025-09-28T01:54:16.601923Z","end":"2025-09-28T01:54:18.563958Z","steps":["trace[934563775] 'process raft request'  (duration: 539.768484ms)","trace[934563775] 'compare'  (duration: 152.726093ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:18.564131Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:16.601907Z","time spent":"696.453622ms","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":732,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller.18694f6c6ed3eae0\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller.18694f6c6ed3eae0\" value_size:642 lease:3238538348504950598 >> failure:<>"}
{"level":"warn","ts":"2025-09-28T01:54:19.410488Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"746.687303ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:19.410534Z","caller":"traceutil/trace.go:172","msg":"trace[1126713621] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13345; }","duration":"746.736899ms","start":"2025-09-28T01:54:18.663789Z","end":"2025-09-28T01:54:19.410526Z","steps":["trace[1126713621] 'range keys from in-memory index tree'  (duration: 746.606382ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:19.410554Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:18.663773Z","time spent":"746.776839ms","remote":"127.0.0.1:48392","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-28T01:54:19.410797Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"502.552422ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:19.410819Z","caller":"traceutil/trace.go:172","msg":"trace[1529067547] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13345; }","duration":"502.575989ms","start":"2025-09-28T01:54:18.908238Z","end":"2025-09-28T01:54:19.410814Z","steps":["trace[1529067547] 'range keys from in-memory index tree'  (duration: 502.531505ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:19.412288Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"280.4855ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538348504950653 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/ingress-nginx/tcp-services.18694f6c6f043078\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/tcp-services.18694f6c6f043078\" value_size:606 lease:3238538348504950598 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-09-28T01:54:19.412338Z","caller":"traceutil/trace.go:172","msg":"trace[1111567430] linearizableReadLoop","detail":"{readStateIndex:16600; appliedIndex:16599; }","duration":"274.585549ms","start":"2025-09-28T01:54:19.137746Z","end":"2025-09-28T01:54:19.412331Z","steps":["trace[1111567430] 'read index received'  (duration: 152.664¬µs)","trace[1111567430] 'applied index is now lower than readState.Index'  (duration: 274.431749ms)"],"step_count":2}
{"level":"info","ts":"2025-09-28T01:54:19.412387Z","caller":"traceutil/trace.go:172","msg":"trace[809483644] transaction","detail":"{read_only:false; response_revision:13346; number_of_response:1; }","duration":"747.235021ms","start":"2025-09-28T01:54:18.665148Z","end":"2025-09-28T01:54:19.412383Z","steps":["trace[809483644] 'process raft request'  (duration: 466.552183ms)","trace[809483644] 'compare'  (duration: 279.273084ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:19.412416Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:18.665124Z","time spent":"747.275813ms","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":684,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/tcp-services.18694f6c6f043078\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/tcp-services.18694f6c6f043078\" value_size:606 lease:3238538348504950598 >> failure:<>"}
{"level":"warn","ts":"2025-09-28T01:54:19.412797Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"275.819355ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:19.412836Z","caller":"traceutil/trace.go:172","msg":"trace[1298459809] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:0; response_revision:13346; }","duration":"275.861283ms","start":"2025-09-28T01:54:19.136967Z","end":"2025-09-28T01:54:19.412828Z","steps":["trace[1298459809] 'agreement among raft nodes before linearized reading'  (duration: 275.786323ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:19.486611Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"823.273667ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" keys_only:true ","response":"range_response_count:71 size:6536"}
{"level":"info","ts":"2025-09-28T01:54:19.487960Z","caller":"traceutil/trace.go:172","msg":"trace[46103625] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:71; response_revision:13345; }","duration":"824.629378ms","start":"2025-09-28T01:54:18.663311Z","end":"2025-09-28T01:54:19.487940Z","steps":["trace[46103625] 'range keys from in-memory index tree'  (duration: 724.753174ms)","trace[46103625] 'range keys from bolt db'  (duration: 98.142099ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:19.489090Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:18.663283Z","time spent":"825.788413ms","remote":"127.0.0.1:49380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":71,"response size":6559,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" keys_only:true "}
{"level":"warn","ts":"2025-09-28T01:54:19.918378Z","caller":"etcdserver/v3_server.go:911","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238538348504950655,"retry-timeout":"500ms"}
{"level":"info","ts":"2025-09-28T01:54:19.925350Z","caller":"traceutil/trace.go:172","msg":"trace[1437080756] linearizableReadLoop","detail":"{readStateIndex:16600; appliedIndex:16600; }","duration":"512.984406ms","start":"2025-09-28T01:54:19.412350Z","end":"2025-09-28T01:54:19.925334Z","steps":["trace[1437080756] 'read index received'  (duration: 512.979106ms)","trace[1437080756] 'applied index is now lower than readState.Index'  (duration: 4.543¬µs)"],"step_count":2}
{"level":"info","ts":"2025-09-28T01:54:19.925751Z","caller":"traceutil/trace.go:172","msg":"trace[1344598438] transaction","detail":"{read_only:false; response_revision:13347; number_of_response:1; }","duration":"554.300251ms","start":"2025-09-28T01:54:19.371426Z","end":"2025-09-28T01:54:19.925726Z","steps":["trace[1344598438] 'process raft request'  (duration: 554.185162ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:19.925839Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.371404Z","time spent":"554.395654ms","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":821,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller-9cc49f96f-7ldqn.18694f6d12f33f7a\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller-9cc49f96f-7ldqn.18694f6d12f33f7a\" value_size:715 lease:3238538348504950598 >> failure:<>"}
{"level":"warn","ts":"2025-09-28T01:54:19.925977Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"514.947743ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:19.928403Z","caller":"traceutil/trace.go:172","msg":"trace[1620614268] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13347; }","duration":"517.545778ms","start":"2025-09-28T01:54:19.410837Z","end":"2025-09-28T01:54:19.928383Z","steps":["trace[1620614268] 'agreement among raft nodes before linearized reading'  (duration: 514.867862ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:20.557613Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"762.828338ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:20.557680Z","caller":"traceutil/trace.go:172","msg":"trace[741939263] range","detail":"{range_begin:/registry/leases; range_end:; response_count:0; response_revision:13347; }","duration":"762.897808ms","start":"2025-09-28T01:54:19.794771Z","end":"2025-09-28T01:54:20.557669Z","steps":["trace[741939263] 'agreement among raft nodes before linearized reading'  (duration: 230.070855ms)","trace[741939263] 'range keys from in-memory index tree'  (duration: 532.729372ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:20.557707Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.794759Z","time spent":"762.942765ms","remote":"127.0.0.1:48836","response type":"/etcdserverpb.KV/Range","request count":0,"request size":20,"response count":0,"response size":28,"request content":"key:\"/registry/leases\" limit:1 "}
{"level":"warn","ts":"2025-09-28T01:54:20.557946Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"825.88959ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-09-28T01:54:20.557984Z","caller":"traceutil/trace.go:172","msg":"trace[1984025574] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:13347; }","duration":"825.913062ms","start":"2025-09-28T01:54:19.732049Z","end":"2025-09-28T01:54:20.557962Z","steps":["trace[1984025574] 'agreement among raft nodes before linearized reading'  (duration: 292.817409ms)","trace[1984025574] 'range keys from in-memory index tree'  (duration: 532.996748ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:20.558002Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.732033Z","time spent":"825.964171ms","remote":"127.0.0.1:48400","response type":"/etcdserverpb.KV/Range","request count":0,"request size":39,"response count":1,"response size":154,"request content":"key:\"/registry/masterleases/192.168.58.2\" limit:1 "}
{"level":"warn","ts":"2025-09-28T01:54:20.558140Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"1.132034774s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:20.558161Z","caller":"traceutil/trace.go:172","msg":"trace[1533877480] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13347; }","duration":"1.132054933s","start":"2025-09-28T01:54:19.426100Z","end":"2025-09-28T01:54:20.558155Z","steps":["trace[1533877480] 'agreement among raft nodes before linearized reading'  (duration: 598.776708ms)","trace[1533877480] 'range keys from in-memory index tree'  (duration: 533.245761ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:20.558177Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.426086Z","time spent":"1.132088059s","remote":"127.0.0.1:48374","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-28T01:54:20.558739Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"533.784298ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538348504950656 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/ingress-nginx/udp-services.18694f6c6f04a784\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/udp-services.18694f6c6f04a784\" value_size:606 lease:3238538348504950598 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-09-28T01:54:20.558870Z","caller":"traceutil/trace.go:172","msg":"trace[723020202] transaction","detail":"{read_only:false; response_revision:13350; number_of_response:1; }","duration":"505.35546ms","start":"2025-09-28T01:54:20.053508Z","end":"2025-09-28T01:54:20.558864Z","steps":["trace[723020202] 'process raft request'  (duration: 505.316466ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:20.558902Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:20.053493Z","time spent":"505.393602ms","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":781,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller-9cc49f96f-7ldqn.18694f6d3c8ebe20\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller-9cc49f96f-7ldqn.18694f6d3c8ebe20\" value_size:675 lease:3238538348504950598 >> failure:<>"}
{"level":"info","ts":"2025-09-28T01:54:20.559045Z","caller":"traceutil/trace.go:172","msg":"trace[501614812] transaction","detail":"{read_only:false; response_revision:13348; number_of_response:1; }","duration":"837.97116ms","start":"2025-09-28T01:54:19.721068Z","end":"2025-09-28T01:54:20.559039Z","steps":["trace[501614812] 'process raft request'  (duration: 303.864876ms)","trace[501614812] 'compare'  (duration: 533.430321ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:20.559071Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.721021Z","time spent":"838.038453ms","remote":"127.0.0.1:48480","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":684,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/udp-services.18694f6c6f04a784\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/udp-services.18694f6c6f04a784\" value_size:606 lease:3238538348504950598 >> failure:<>"}
{"level":"info","ts":"2025-09-28T01:54:20.559209Z","caller":"traceutil/trace.go:172","msg":"trace[393888344] transaction","detail":"{read_only:false; response_revision:13349; number_of_response:1; }","duration":"837.927055ms","start":"2025-09-28T01:54:19.721277Z","end":"2025-09-28T01:54:20.559204Z","steps":["trace[393888344] 'process raft request'  (duration: 837.508719ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:20.559251Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:19.721268Z","time spent":"837.95488ms","remote":"127.0.0.1:48836","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":483,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" mod_revision:0 > success:<request_put:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" value_size:424 >> failure:<>"}
{"level":"info","ts":"2025-09-28T01:54:20.559337Z","caller":"traceutil/trace.go:172","msg":"trace[784665954] linearizableReadLoop","detail":"{readStateIndex:16603; appliedIndex:16601; }","duration":"534.546766ms","start":"2025-09-28T01:54:20.024785Z","end":"2025-09-28T01:54:20.559332Z","steps":["trace[784665954] 'read index received'  (duration: 32.842¬µs)","trace[784665954] 'applied index is now lower than readState.Index'  (duration: 534.513451ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:20.559368Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"626.334139ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:20.559425Z","caller":"traceutil/trace.go:172","msg":"trace[1888554474] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13350; }","duration":"626.35004ms","start":"2025-09-28T01:54:19.933029Z","end":"2025-09-28T01:54:20.559379Z","steps":["trace[1888554474] 'agreement among raft nodes before linearized reading'  (duration: 626.322972ms)"],"step_count":1}
{"level":"info","ts":"2025-09-28T01:54:20.806205Z","caller":"traceutil/trace.go:172","msg":"trace[1254669144] linearizableReadLoop","detail":"{readStateIndex:16604; appliedIndex:16604; }","duration":"214.634366ms","start":"2025-09-28T01:54:20.591538Z","end":"2025-09-28T01:54:20.806173Z","steps":["trace[1254669144] 'read index received'  (duration: 214.622914ms)","trace[1254669144] 'applied index is now lower than readState.Index'  (duration: 9.748¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:21.262918Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"281.76417ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:21.263123Z","caller":"traceutil/trace.go:172","msg":"trace[1847803278] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13350; }","duration":"281.976909ms","start":"2025-09-28T01:54:20.981138Z","end":"2025-09-28T01:54:21.263115Z","steps":["trace[1847803278] 'range keys from in-memory index tree'  (duration: 281.72939ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:21.263275Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"671.733719ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-28T01:54:21.263299Z","caller":"traceutil/trace.go:172","msg":"trace[1616982419] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13350; }","duration":"671.755534ms","start":"2025-09-28T01:54:20.591536Z","end":"2025-09-28T01:54:21.263291Z","steps":["trace[1616982419] 'agreement among raft nodes before linearized reading'  (duration: 215.126052ms)","trace[1616982419] 'range keys from in-memory index tree'  (duration: 456.582216ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:21.263444Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:20.591524Z","time spent":"671.914386ms","remote":"127.0.0.1:48392","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-28T01:54:21.263530Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"457.054221ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538348504950663 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:2cf199782e1e2786>","response":"size:40"}
{"level":"info","ts":"2025-09-28T01:54:21.263681Z","caller":"traceutil/trace.go:172","msg":"trace[369579405] linearizableReadLoop","detail":"{readStateIndex:16605; appliedIndex:16604; }","duration":"328.734234ms","start":"2025-09-28T01:54:20.934941Z","end":"2025-09-28T01:54:21.263676Z","steps":["trace[369579405] 'read index received'  (duration: 24.513¬µs)","trace[369579405] 'applied index is now lower than readState.Index'  (duration: 328.709257ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-28T01:54:21.263799Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:20.576978Z","time spent":"686.819335ms","remote":"127.0.0.1:48400","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2025-09-28T01:54:21.264394Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"329.394072ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/\" range_end:\"/registry/clusterrolebindings0\" limit:10000 revision:13350 ","response":"range_response_count:60 size:44020"}
{"level":"info","ts":"2025-09-28T01:54:21.264422Z","caller":"traceutil/trace.go:172","msg":"trace[1800898469] range","detail":"{range_begin:/registry/clusterrolebindings/; range_end:/registry/clusterrolebindings0; response_count:60; response_revision:13350; }","duration":"329.492185ms","start":"2025-09-28T01:54:20.934924Z","end":"2025-09-28T01:54:21.264416Z","steps":["trace[1800898469] 'agreement among raft nodes before linearized reading'  (duration: 329.016252ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-28T01:54:21.264439Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-28T01:54:20.934895Z","time spent":"329.538033ms","remote":"127.0.0.1:49064","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":60,"response size":44043,"request content":"key:\"/registry/clusterrolebindings/\" range_end:\"/registry/clusterrolebindings0\" limit:10000 revision:13350 "}
{"level":"info","ts":"2025-09-28T01:54:21.709257Z","caller":"traceutil/trace.go:172","msg":"trace[242052464] linearizableReadLoop","detail":"{readStateIndex:16605; appliedIndex:16605; }","duration":"445.552597ms","start":"2025-09-28T01:54:21.263694Z","end":"2025-09-28T01:54:21.709247Z","steps":["trace[242052464] 'read index received'  (duration: 445.548328ms)","trace[242052464] 'applied index is now lower than readState.Index'  (duration: 3.62¬µs)"],"step_count":2}


==> etcd [f24c63598434] <==
{"level":"warn","ts":"2025-09-17T11:38:16.285218Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"182.951386ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:16.285314Z","caller":"traceutil/trace.go:172","msg":"trace[593309961] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3725; }","duration":"183.073466ms","start":"2025-09-17T11:38:16.102219Z","end":"2025-09-17T11:38:16.285292Z","steps":["trace[593309961] 'agreement among raft nodes before linearized reading'  (duration: 182.899964ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:16.521479Z","caller":"traceutil/trace.go:172","msg":"trace[101683160] linearizableReadLoop","detail":"{readStateIndex:4543; appliedIndex:4543; }","duration":"232.694798ms","start":"2025-09-17T11:38:16.288764Z","end":"2025-09-17T11:38:16.521458Z","steps":["trace[101683160] 'read index received'  (duration: 232.688321ms)","trace[101683160] 'applied index is now lower than readState.Index'  (duration: 5.594¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:16.939476Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"650.704558ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:16.939549Z","caller":"traceutil/trace.go:172","msg":"trace[1209061992] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3725; }","duration":"650.775509ms","start":"2025-09-17T11:38:16.288757Z","end":"2025-09-17T11:38:16.939532Z","steps":["trace[1209061992] 'agreement among raft nodes before linearized reading'  (duration: 232.785279ms)","trace[1209061992] 'range keys from in-memory index tree'  (duration: 417.898278ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:16.939577Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:16.288727Z","time spent":"650.8444ms","remote":"127.0.0.1:48794","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-17T11:38:16.939912Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"418.301907ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432689962 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:3718 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238538200432689958 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-17T11:38:16.939986Z","caller":"traceutil/trace.go:172","msg":"trace[1893097651] transaction","detail":"{read_only:false; response_revision:3726; number_of_response:1; }","duration":"652.245959ms","start":"2025-09-17T11:38:16.287733Z","end":"2025-09-17T11:38:16.939979Z","steps":["trace[1893097651] 'process raft request'  (duration: 233.830216ms)","trace[1893097651] 'compare'  (duration: 418.178453ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:16.940030Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:16.287694Z","time spent":"652.319855ms","remote":"127.0.0.1:47806","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:3718 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238538200432689958 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >"}
{"level":"info","ts":"2025-09-17T11:38:16.978994Z","caller":"traceutil/trace.go:172","msg":"trace[63865330] linearizableReadLoop","detail":"{readStateIndex:4544; appliedIndex:4544; }","duration":"230.154385ms","start":"2025-09-17T11:38:16.748807Z","end":"2025-09-17T11:38:16.978962Z","steps":["trace[63865330] 'read index received'  (duration: 230.138192ms)","trace[63865330] 'applied index is now lower than readState.Index'  (duration: 14.524¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:16.979419Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"230.601192ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-09-17T11:38:16.979557Z","caller":"traceutil/trace.go:172","msg":"trace[1368566471] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3726; }","duration":"230.755853ms","start":"2025-09-17T11:38:16.748782Z","end":"2025-09-17T11:38:16.979538Z","steps":["trace[1368566471] 'agreement among raft nodes before linearized reading'  (duration: 230.42926ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:17.234295Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"255.119073ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432689963 > lease_revoke:<id:2cf19955b4527694>","response":"size:28"}
{"level":"info","ts":"2025-09-17T11:38:17.235909Z","caller":"traceutil/trace.go:172","msg":"trace[1839542662] transaction","detail":"{read_only:false; response_revision:3728; number_of_response:1; }","duration":"244.53485ms","start":"2025-09-17T11:38:16.991347Z","end":"2025-09-17T11:38:17.235882Z","steps":["trace[1839542662] 'process raft request'  (duration: 243.756446ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:17.235924Z","caller":"traceutil/trace.go:172","msg":"trace[2057736751] linearizableReadLoop","detail":"{readStateIndex:4545; appliedIndex:4544; }","duration":"256.676864ms","start":"2025-09-17T11:38:16.979159Z","end":"2025-09-17T11:38:17.235836Z","steps":["trace[2057736751] 'read index received'  (duration: 236.898¬µs)","trace[2057736751] 'applied index is now lower than readState.Index'  (duration: 256.43555ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:17.236180Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"293.288966ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-09-17T11:38:17.239531Z","caller":"traceutil/trace.go:172","msg":"trace[223241905] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:3728; }","duration":"296.156462ms","start":"2025-09-17T11:38:16.942864Z","end":"2025-09-17T11:38:17.239020Z","steps":["trace[223241905] 'agreement among raft nodes before linearized reading'  (duration: 293.13931ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:17.504512Z","caller":"traceutil/trace.go:172","msg":"trace[1110277838] linearizableReadLoop","detail":"{readStateIndex:4546; appliedIndex:4546; }","duration":"265.695053ms","start":"2025-09-17T11:38:17.238782Z","end":"2025-09-17T11:38:17.504477Z","steps":["trace[1110277838] 'read index received'  (duration: 265.684258ms)","trace[1110277838] 'applied index is now lower than readState.Index'  (duration: 9.028¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:17.618073Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"515.960872ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:17.618215Z","caller":"traceutil/trace.go:172","msg":"trace[2145839010] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3728; }","duration":"516.121617ms","start":"2025-09-17T11:38:17.102066Z","end":"2025-09-17T11:38:17.618187Z","steps":["trace[2145839010] 'agreement among raft nodes before linearized reading'  (duration: 402.609478ms)","trace[2145839010] 'range keys from in-memory index tree'  (duration: 113.319796ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:17.619048Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"113.648645ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432689967 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:3720 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-17T11:38:17.619183Z","caller":"traceutil/trace.go:172","msg":"trace[2119588223] linearizableReadLoop","detail":"{readStateIndex:4547; appliedIndex:4546; }","duration":"114.536666ms","start":"2025-09-17T11:38:17.504626Z","end":"2025-09-17T11:38:17.619163Z","steps":["trace[2119588223] 'read index received'  (duration: 63.886¬µs)","trace[2119588223] 'applied index is now lower than readState.Index'  (duration: 114.470817ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:17.619501Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"370.613534ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-09-17T11:38:17.619568Z","caller":"traceutil/trace.go:172","msg":"trace[444603598] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:3729; }","duration":"370.683602ms","start":"2025-09-17T11:38:17.248867Z","end":"2025-09-17T11:38:17.619551Z","steps":["trace[444603598] 'agreement among raft nodes before linearized reading'  (duration: 370.365155ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:17.619624Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:17.248838Z","time spent":"370.77212ms","remote":"127.0.0.1:47806","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":1,"response size":154,"request content":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" "}
{"level":"info","ts":"2025-09-17T11:38:17.619965Z","caller":"traceutil/trace.go:172","msg":"trace[2028985584] transaction","detail":"{read_only:false; response_revision:3729; number_of_response:1; }","duration":"434.83329ms","start":"2025-09-17T11:38:17.185110Z","end":"2025-09-17T11:38:17.619943Z","steps":["trace[2028985584] 'process raft request'  (duration: 319.510662ms)","trace[2028985584] 'compare'  (duration: 113.406743ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:17.620900Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:17.185070Z","time spent":"435.66135ms","remote":"127.0.0.1:48260","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:3720 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-09-17T11:38:18.310903Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"116.959024ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:18.310961Z","caller":"traceutil/trace.go:172","msg":"trace[2092936637] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3729; }","duration":"117.019376ms","start":"2025-09-17T11:38:18.193931Z","end":"2025-09-17T11:38:18.310950Z","steps":["trace[2092936637] 'range keys from in-memory index tree'  (duration: 116.912901ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:18.310980Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"528.867748ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:18.311013Z","caller":"traceutil/trace.go:172","msg":"trace[247143385] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3729; }","duration":"528.909651ms","start":"2025-09-17T11:38:17.782096Z","end":"2025-09-17T11:38:18.311006Z","steps":["trace[247143385] 'range keys from in-memory index tree'  (duration: 528.676876ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:18.311034Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:17.782063Z","time spent":"528.96598ms","remote":"127.0.0.1:47756","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-17T11:38:18.310981Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"209.843638ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:18.311156Z","caller":"traceutil/trace.go:172","msg":"trace[1046254840] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3729; }","duration":"209.971409ms","start":"2025-09-17T11:38:18.101127Z","end":"2025-09-17T11:38:18.311098Z","steps":["trace[1046254840] 'range keys from in-memory index tree'  (duration: 209.814196ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:19.353310Z","caller":"traceutil/trace.go:172","msg":"trace[35282094] transaction","detail":"{read_only:false; response_revision:3730; number_of_response:1; }","duration":"103.010253ms","start":"2025-09-17T11:38:19.250285Z","end":"2025-09-17T11:38:19.353295Z","steps":["trace[35282094] 'process raft request'  (duration: 102.910842ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:19.687346Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"169.339609ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:19.687571Z","caller":"traceutil/trace.go:172","msg":"trace[1531439806] range","detail":"{range_begin:/registry/controllers; range_end:; response_count:0; response_revision:3730; }","duration":"169.568361ms","start":"2025-09-17T11:38:19.517979Z","end":"2025-09-17T11:38:19.687547Z","steps":["trace[1531439806] 'range keys from in-memory index tree'  (duration: 169.200848ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:21.461942Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"106.636821ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432689995 > lease_revoke:<id:2cf19955b452c2f1>","response":"size:28"}
{"level":"info","ts":"2025-09-17T11:38:21.462030Z","caller":"traceutil/trace.go:172","msg":"trace[339490850] linearizableReadLoop","detail":"{readStateIndex:4549; appliedIndex:4548; }","duration":"103.674821ms","start":"2025-09-17T11:38:21.358345Z","end":"2025-09-17T11:38:21.462020Z","steps":["trace[339490850] 'read index received'  (duration: 60.549¬µs)","trace[339490850] 'applied index is now lower than readState.Index'  (duration: 103.613487ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:21.462138Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"103.787479ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-09-17T11:38:21.462156Z","caller":"traceutil/trace.go:172","msg":"trace[1380155828] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3730; }","duration":"103.811719ms","start":"2025-09-17T11:38:21.358340Z","end":"2025-09-17T11:38:21.462151Z","steps":["trace[1380155828] 'agreement among raft nodes before linearized reading'  (duration: 103.713192ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:23.240051Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"137.026956ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:23.240152Z","caller":"traceutil/trace.go:172","msg":"trace[634146073] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3732; }","duration":"137.139389ms","start":"2025-09-17T11:38:23.102992Z","end":"2025-09-17T11:38:23.240132Z","steps":["trace[634146073] 'agreement among raft nodes before linearized reading'  (duration: 61.819011ms)","trace[634146073] 'range keys from in-memory index tree'  (duration: 75.177281ms)"],"step_count":2}
{"level":"info","ts":"2025-09-17T11:38:23.240484Z","caller":"traceutil/trace.go:172","msg":"trace[1315861001] transaction","detail":"{read_only:false; response_revision:3733; number_of_response:1; }","duration":"185.840133ms","start":"2025-09-17T11:38:23.054625Z","end":"2025-09-17T11:38:23.240465Z","steps":["trace[1315861001] 'process raft request'  (duration: 110.238671ms)","trace[1315861001] 'compare'  (duration: 75.165684ms)"],"step_count":2}
{"level":"info","ts":"2025-09-17T11:38:23.700018Z","caller":"traceutil/trace.go:172","msg":"trace[2037667021] transaction","detail":"{read_only:false; response_revision:3734; number_of_response:1; }","duration":"154.490721ms","start":"2025-09-17T11:38:23.545512Z","end":"2025-09-17T11:38:23.700003Z","steps":["trace[2037667021] 'process raft request'  (duration: 154.302021ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:27.957686Z","caller":"traceutil/trace.go:172","msg":"trace[375575823] transaction","detail":"{read_only:false; response_revision:3737; number_of_response:1; }","duration":"210.599109ms","start":"2025-09-17T11:38:27.747071Z","end":"2025-09-17T11:38:27.957670Z","steps":["trace[375575823] 'process raft request'  (duration: 210.448838ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-17T11:38:28.418506Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"131.670537ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432690037 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:3729 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-17T11:38:28.418607Z","caller":"traceutil/trace.go:172","msg":"trace[1012104639] transaction","detail":"{read_only:false; response_revision:3738; number_of_response:1; }","duration":"234.631886ms","start":"2025-09-17T11:38:27.878904Z","end":"2025-09-17T11:38:28.418596Z","steps":["trace[1012104639] 'process raft request'  (duration: 102.812944ms)","trace[1012104639] 'compare'  (duration: 131.533337ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:37.082630Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"172.295921ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-17T11:38:37.082700Z","caller":"traceutil/trace.go:172","msg":"trace[1926864670] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3743; }","duration":"172.373263ms","start":"2025-09-17T11:38:36.910317Z","end":"2025-09-17T11:38:37.082690Z","steps":["trace[1926864670] 'agreement among raft nodes before linearized reading'  (duration: 52.866926ms)","trace[1926864670] 'range keys from in-memory index tree'  (duration: 119.414763ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:37.083142Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"119.738758ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432690075 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:3736 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238538200432690073 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-17T11:38:37.083487Z","caller":"traceutil/trace.go:172","msg":"trace[1954088721] transaction","detail":"{read_only:false; response_revision:3744; number_of_response:1; }","duration":"177.596051ms","start":"2025-09-17T11:38:36.905877Z","end":"2025-09-17T11:38:37.083474Z","steps":["trace[1954088721] 'process raft request'  (duration: 57.325317ms)","trace[1954088721] 'compare'  (duration: 119.349395ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:37.574229Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"209.383184ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432690083 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3743 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-17T11:38:37.574355Z","caller":"traceutil/trace.go:172","msg":"trace[981510961] transaction","detail":"{read_only:false; response_revision:3745; number_of_response:1; }","duration":"430.766351ms","start":"2025-09-17T11:38:37.143570Z","end":"2025-09-17T11:38:37.574337Z","steps":["trace[981510961] 'process raft request'  (duration: 221.199724ms)","trace[981510961] 'compare'  (duration: 208.713994ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-17T11:38:37.574432Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-09-17T11:38:37.143554Z","time spent":"430.83967ms","remote":"127.0.0.1:48048","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3743 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-09-17T11:38:39.332549Z","caller":"traceutil/trace.go:172","msg":"trace[2028335021] transaction","detail":"{read_only:false; response_revision:3746; number_of_response:1; }","duration":"134.302826ms","start":"2025-09-17T11:38:39.198234Z","end":"2025-09-17T11:38:39.332537Z","steps":["trace[2028335021] 'process raft request'  (duration: 134.22941ms)"],"step_count":1}
{"level":"info","ts":"2025-09-17T11:38:42.233932Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-09-17T11:38:42.234314Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"error","ts":"2025-09-17T11:38:42.235234Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"warn","ts":"2025-09-17T11:38:42.578645Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"287.6541ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238538200432690103 > lease_revoke:<id:2cf19955b452c362>","response":"size:28"}


==> kernel <==
 01:54:39 up  7:31,  0 users,  load average: 31.46, 30.56, 19.31
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [2dde68910429] <==
E0926 13:27:54.427405       1 controller.go:131] Unable to remove endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registry/masterleases/192.168.58.2, ResourceVersion: 0, AdditionalErrorMsg: <nil>
I0926 13:27:54.428155       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0926 13:27:54.429327       1 storage_flowcontrol.go:172] APF bootstrap ensurer is exiting
I0926 13:27:54.429477       1 cluster_authentication_trust_controller.go:482] Shutting down cluster_authentication_trust_controller controller
I0926 13:27:54.430109       1 apiservice_controller.go:134] Shutting down APIServiceRegistrationController
I0926 13:27:54.430162       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I0926 13:27:54.430190       1 apf_controller.go:389] Shutting down API Priority and Fairness config worker
I0926 13:27:54.430400       1 controller.go:132] Ending legacy_token_tracking_controller
I0926 13:27:54.430424       1 controller.go:133] Shutting down legacy_token_tracking_controller
I0926 13:27:54.430478       1 customresource_discovery_controller.go:332] Shutting down DiscoveryController
I0926 13:27:54.430520       1 crd_finalizer.go:281] Shutting down CRDFinalizer
I0926 13:27:54.430551       1 apiapproval_controller.go:201] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0926 13:27:54.430645       1 local_available_controller.go:172] Shutting down LocalAvailability controller
I0926 13:27:54.430692       1 remote_available_controller.go:441] Shutting down RemoteAvailability controller
I0926 13:27:54.430721       1 autoregister_controller.go:168] Shutting down autoregister controller
I0926 13:27:54.430756       1 establishing_controller.go:92] Shutting down EstablishingController
I0926 13:27:54.430782       1 naming_controller.go:310] Shutting down NamingConditionController
I0926 13:27:54.430807       1 controller.go:120] Shutting down OpenAPI V3 controller
I0926 13:27:54.430835       1 controller.go:170] Shutting down OpenAPI controller
I0926 13:27:54.430913       1 system_namespaces_controller.go:76] Shutting down system namespaces controller
I0926 13:27:54.431076       1 nonstructuralschema_controller.go:207] Shutting down NonStructuralSchemaConditionController
I0926 13:27:54.435654       1 controller.go:157] Shutting down quota evaluator
I0926 13:27:54.435775       1 controller.go:176] quota evaluator worker shutdown
I0926 13:27:54.436311       1 controller.go:176] quota evaluator worker shutdown
I0926 13:27:54.436357       1 controller.go:176] quota evaluator worker shutdown
I0926 13:27:54.436373       1 controller.go:176] quota evaluator worker shutdown
I0926 13:27:54.436386       1 controller.go:176] quota evaluator worker shutdown
I0926 13:27:54.441817       1 crdregistration_controller.go:145] Shutting down crd-autoregister controller
I0926 13:27:54.442851       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0926 13:27:54.443275       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0926 13:27:54.443794       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I0926 13:27:54.443838       1 controller.go:84] Shutting down OpenAPI AggregationController
I0926 13:27:54.443880       1 object_count_tracker.go:141] "StorageObjectCountTracker pruner is exiting"
I0926 13:27:54.443901       1 dynamic_serving_content.go:149] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0926 13:27:54.443955       1 repairip.go:246] Shutting down ipallocator-repair-controller
I0926 13:27:54.445442       1 secure_serving.go:259] Stopped listening on [::]:8443
I0926 13:27:54.445496       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0926 13:27:54.446072       1 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0926 13:27:54.446603       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
E0926 13:27:54.451038       1 storage_rbac.go:264] "Unhandled Error" err="unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:ttl-after-finished-controller: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:ttl-after-finished-controller\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.451583       1 storage_rbac.go:264] "Unhandled Error" err="unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:root-ca-cert-publisher: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:root-ca-cert-publisher\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.453164       1 storage_rbac.go:264] "Unhandled Error" err="unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:validatingadmissionpolicy-status-controller: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:validatingadmissionpolicy-status-controller\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.454000       1 storage_rbac.go:264] "Unhandled Error" err="unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:legacy-service-account-token-cleaner: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:legacy-service-account-token-cleaner\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.455887       1 storage_rbac.go:264] "Unhandled Error" err="unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:selinux-warning-controller: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:selinux-warning-controller\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.456535       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/extension-apiserver-authentication-reader in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/extension-apiserver-authentication-reader\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.459616       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:bootstrap-signer\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.462076       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system:controller:cloud-provider in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:cloud-provider\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.462472       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:token-cleaner\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.464103       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system::leader-locking-kube-controller-manager in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-controller-manager\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.464497       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system::leader-locking-kube-scheduler in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-scheduler\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.466138       1 storage_rbac.go:295] "Unhandled Error" err="unable to reconcile role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/roles/system:controller:bootstrap-signer\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.466568       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system::extension-apiserver-authentication-reader in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::extension-apiserver-authentication-reader\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.468528       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-controller-manager in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-controller-manager\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.469171       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-scheduler in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-scheduler\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.471094       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:bootstrap-signer\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.471345       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:cloud-provider in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:cloud-provider\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.473013       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:token-cleaner\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.474487       1 storage_rbac.go:329] "Unhandled Error" err="unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/rolebindings/system:controller:bootstrap-signer\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
E0926 13:27:54.477535       1 storage_rbac.go:187] "Unhandled Error" err="unable to initialize clusterroles: Get \"https://[::1]:8443/apis/rbac.authorization.k8s.io/v1/clusterroles\": dial tcp [::1]:8443: connect: connection refused" logger="UnhandledError"
F0926 13:27:54.477571       1 hooks.go:204] PostStartHook "rbac/bootstrap-roles" failed: unable to initialize roles: timed out waiting for the condition


==> kube-apiserver [aba039da04cf] <==
I0928 01:41:31.436515       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
{"level":"warn","ts":"2025-09-28T01:41:44.489973Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:41:47.810170       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.107.56.183"}
I0928 01:41:49.195758       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.103.206.143"}
I0928 01:41:50.868716       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0928 01:41:55.747477       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:42:11.176683       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:43:16.519381       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:43:30.719325       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:43:43.696561Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:44:50.415171       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:44:55.063230       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:44:55.136576Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:46:22.546615       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:46:30.921130       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:47:39.302269       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:47:58.582778       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:48:00.670813Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:48:15.637647Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:48:19.837801Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:48:46.528160       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:48:50.259725       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
{"level":"warn","ts":"2025-09-28T01:49:07.195189Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:49:15.518762       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:50:11.577962       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:50:23.987133       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:51:28.970111       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:51:41.763757Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:51:57.229636Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:51:59.607081       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0928 01:52:39.719682       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:52:42.373841Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:52:46.064908Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:52:57.830336Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:00.897610Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:02.962032Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0011d12c0/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
E0928 01:53:03.031513       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
{"level":"warn","ts":"2025-09-28T01:53:04.556727Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:04.557278Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:07.577650Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:07.577981Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0014b05a0/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E0928 01:53:07.578176       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0928 01:53:07.578195       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0928 01:53:07.578278       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 3.467¬µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0928 01:53:07.580349       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0928 01:53:07.580931       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="2.620655ms" method="POST" path="/api/v1/namespaces/kube-system/events" result=null
I0928 01:53:10.879958       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:53:11.337890Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:53:14.250385       1 stats.go:136] "Error getting keys" err="Timeout: Too large resource version: 13297, current: 13295"
{"level":"warn","ts":"2025-09-28T01:53:15.041952Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:53:15.137653Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c74f00/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Unavailable desc = etcdserver: request timed out"}
{"level":"warn","ts":"2025-09-28T01:53:15.272309Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
E0928 01:53:15.715618       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: etcdserver: request timed out" logger="UnhandledError"
{"level":"warn","ts":"2025-09-28T01:53:18.114159Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:53:59.063589       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:54:06.149642Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:54:30.397778Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0928 01:54:42.698017       1 stats.go:136] "Error getting keys" err="empty key: \"\""
{"level":"warn","ts":"2025-09-28T01:54:46.178689Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000638780/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2025-09-28T01:54:52.254004Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000c741e0/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}


==> kube-controller-manager [171c42f73e29] <==
I0917 03:26:22.047751       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0917 03:26:22.402022       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0917 03:26:22.423114       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0917 03:26:22.433339       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0917 03:26:22.434598       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0917 03:26:22.434867       1 shared_informer.go:356] "Caches are synced" controller="node"
I0917 03:26:22.434932       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0917 03:26:22.434965       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0917 03:26:22.434973       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0917 03:26:22.434979       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0917 03:26:22.435006       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0917 03:26:22.436715       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0917 03:26:22.436765       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0917 03:26:22.553099       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0917 03:26:22.556231       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0917 03:26:22.556250       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0917 03:26:22.556275       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0917 03:26:22.556285       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0917 03:26:22.556296       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0917 03:26:22.556392       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0917 03:26:22.556411       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0917 03:26:22.556484       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0917 03:26:22.556527       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0917 03:26:22.556537       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0917 03:26:22.556553       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0917 03:26:22.556571       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0917 03:26:22.556590       1 shared_informer.go:356] "Caches are synced" controller="job"
I0917 03:26:22.556651       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0917 03:26:22.556658       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0917 03:26:22.556931       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0917 03:26:22.556989       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0917 03:26:22.556275       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0917 03:26:22.557317       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0917 03:26:22.557355       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0917 03:26:22.556285       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0917 03:26:22.557992       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0917 03:26:22.558075       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0917 03:26:22.558095       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0917 03:26:22.556979       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0917 03:26:22.558218       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0917 03:26:22.558312       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0917 03:26:22.558375       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0917 03:26:22.558375       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0917 03:26:22.556530       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0917 03:26:22.559254       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0917 03:26:22.564293       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0917 03:26:22.564404       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0917 03:26:22.566372       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0917 03:26:22.566853       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0917 03:26:22.609693       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0917 03:26:22.648689       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0917 03:26:22.678134       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0917 03:26:23.804159       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0917 03:26:23.804279       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0917 03:26:23.804304       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0917 03:26:23.859569       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0917 03:35:51.483303       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/backend-service" err="EndpointSlice informer cache is out of date"
E0917 11:30:46.614113       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0917 11:30:47.813711       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0917 11:35:37.034412       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-2lv9s" approvedExpiration="1h0m0s"


==> kube-controller-manager [f7e218163a7d] <==
E0926 13:24:50.177195       1 reflector.go:205] "Failed to watch" err="pods is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0926 13:24:50.177257       1 reflector.go:205] "Failed to watch" err="nodes is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0926 13:24:50.177274       1 reflector.go:205] "Failed to watch" err="volumeattributesclasses.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"volumeattributesclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttributesClass"
E0926 13:24:50.177295       1 reflector.go:205] "Failed to watch" err="persistentvolumeclaims is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0926 13:24:50.177314       1 reflector.go:205] "Failed to watch" err="persistentvolumes is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0926 13:24:50.182926       1 reflector.go:205] "Failed to watch" err="secrets is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"secrets\" in API group \"\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Secret"
E0926 13:24:50.183327       1 reflector.go:205] "Failed to watch" err="networkpolicies.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"networkpolicies\" in API group \"networking.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.NetworkPolicy"
E0926 13:24:50.183796       1 reflector.go:205] "Failed to watch" err="serviceaccounts is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"serviceaccounts\" in API group \"\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceAccount"
E0926 13:24:50.184963       1 reflector.go:205] "Failed to watch" err="volumeattachments.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0926 13:24:50.185412       1 reflector.go:205] "Failed to watch" err="services is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"services\" in API group \"\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0926 13:24:50.196414       1 reflector.go:205] "Failed to watch" err="clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRoleBinding"
E0926 13:24:50.582016       1 reflector.go:205] "Failed to watch" err="resourceslices.resource.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0926 13:24:50.582063       1 reflector.go:205] "Failed to watch" err="replicationcontrollers is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0926 13:24:50.582205       1 reflector.go:205] "Failed to watch" err="validatingadmissionpolicies.admissionregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"validatingadmissionpolicies\" in API group \"admissionregistration.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicy"
E0926 13:24:50.582245       1 reflector.go:205] "Failed to watch" err="resourceclaimtemplates.resource.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"resourceclaimtemplates\" in API group \"resource.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaimTemplate"
E0926 13:24:50.582538       1 reflector.go:205] "Failed to watch" err="apiservices.apiregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"apiservices\" in API group \"apiregistration.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0926 13:24:50.582586       1 reflector.go:205] "Failed to watch" err="certificatesigningrequests.certificates.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"certificatesigningrequests\" in API group \"certificates.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CertificateSigningRequest"
E0926 13:24:50.582618       1 reflector.go:205] "Failed to watch" err="replicasets.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0926 13:24:50.582654       1 reflector.go:205] "Failed to watch" err="csidrivers.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0926 13:24:50.582686       1 reflector.go:205] "Failed to watch" err="runtimeclasses.node.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"runtimeclasses\" in API group \"node.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
E0926 13:24:50.582725       1 reflector.go:205] "Failed to watch" err="leases.coordination.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"leases\" in API group \"coordination.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Lease"
E0926 13:24:50.582751       1 reflector.go:205] "Failed to watch" err="jobs.batch is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"jobs\" in API group \"batch\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Job"
E0926 13:24:50.582786       1 reflector.go:205] "Failed to watch" err="csinodes.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0926 13:24:50.582859       1 reflector.go:205] "Failed to watch" err="clusterroles.rbac.authorization.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRole"
E0926 13:24:50.582891       1 reflector.go:205] "Failed to watch" err="deviceclasses.resource.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope - error from a previous attempt: net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0926 13:24:50.583209       1 reflector.go:205] "Failed to watch" err="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"prioritylevelconfigurations\" in API group \"flowcontrol.apiserver.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-controller-manager\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityLevelConfiguration"
E0926 13:24:50.583248       1 reflector.go:205] "Failed to watch" err="horizontalpodautoscalers.autoscaling is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"horizontalpodautoscalers\" in API group \"autoscaling\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-controller-manager\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v2.HorizontalPodAutoscaler"
E0926 13:24:50.711642       1 reflector.go:205] "Failed to watch" err="customresourcedefinitions.apiextensions.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"customresourcedefinitions\" in API group \"apiextensions.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
I0926 13:25:03.546998       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/storage-provisioner" err="Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again"
E0926 13:25:18.089134       1 node_lifecycle_controller.go:747] "Unhandled Error" err="unable to mark all pods NotReady on node minikube: Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again; queuing for retry" logger="UnhandledError"
I0926 13:25:18.089738       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
E0926 13:25:24.126445       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0926 13:25:29.297217       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/etcd-minikube" err="Operation cannot be fulfilled on pods \"etcd-minikube\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:31.254910       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/storage-provisioner" err="Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:35.124721       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="default/frontend-5d48899665-slk9d" err="Operation cannot be fulfilled on pods \"frontend-5d48899665-slk9d\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:38.120725       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-proxy-qxgs4" err="Operation cannot be fulfilled on pods \"kube-proxy-qxgs4\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:38.137561       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="default/backend-7b6cd5cbc5-b2tgj" err="Operation cannot be fulfilled on pods \"backend-7b6cd5cbc5-b2tgj\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:39.726950       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-controller-manager-minikube" err="Operation cannot be fulfilled on pods \"kube-controller-manager-minikube\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:39.736763       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-scheduler-minikube" err="Operation cannot be fulfilled on pods \"kube-scheduler-minikube\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:25:41.204578       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/coredns-66bc5c9577-djfsq" err="Operation cannot be fulfilled on pods \"coredns-66bc5c9577-djfsq\": the object has been modified; please apply your changes to the latest version and try again"
E0926 13:25:41.231875       1 node_lifecycle_controller.go:747] "Unhandled Error" err="unable to mark all pods NotReady on node minikube: [Operation cannot be fulfilled on pods \"etcd-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"frontend-5d48899665-slk9d\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-proxy-qxgs4\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"backend-7b6cd5cbc5-b2tgj\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-controller-manager-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-scheduler-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"coredns-66bc5c9577-djfsq\": the object has been modified; please apply your changes to the latest version and try again]; queuing for retry" logger="UnhandledError"
I0926 13:25:47.558604       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-scheduler-minikube" err="Operation cannot be fulfilled on pods \"kube-scheduler-minikube\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:09.923103       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/coredns-66bc5c9577-djfsq" err="Operation cannot be fulfilled on pods \"coredns-66bc5c9577-djfsq\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:20.470211       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/etcd-minikube" err="Operation cannot be fulfilled on pods \"etcd-minikube\": the object has been modified; please apply your changes to the latest version and try again"
E0926 13:26:23.295432       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{kube-controller-manager-minikube.1868d7f7a59cdd54  kube-system   9858 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:kube-controller-manager-minikube,UID:900a1f98-0449-4d8e-b94d-268d365affbc,APIVersion:v1,ResourceVersion:3812,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2025-09-26 13:25:13 +0000 UTC,LastTimestamp:2025-09-26 13:25:39.72698203 +0000 UTC m=+6882.393280372,Count:2,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
I0926 13:26:23.911777       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/storage-provisioner" err="Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:24.833912       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="default/frontend-5d48899665-slk9d" err="Operation cannot be fulfilled on pods \"frontend-5d48899665-slk9d\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:28.607311       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-proxy-qxgs4" err="Operation cannot be fulfilled on pods \"kube-proxy-qxgs4\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:32.073608       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="default/backend-7b6cd5cbc5-b2tgj" err="Operation cannot be fulfilled on pods \"backend-7b6cd5cbc5-b2tgj\": the object has been modified; please apply your changes to the latest version and try again"
I0926 13:26:34.309233       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-controller-manager-minikube" err="Operation cannot be fulfilled on pods \"kube-controller-manager-minikube\": the object has been modified; please apply your changes to the latest version and try again"
E0926 13:26:34.309656       1 node_lifecycle_controller.go:747] "Unhandled Error" err="unable to mark all pods NotReady on node minikube: [Operation cannot be fulfilled on pods \"kube-scheduler-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"coredns-66bc5c9577-djfsq\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"etcd-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"storage-provisioner\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"frontend-5d48899665-slk9d\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-proxy-qxgs4\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"backend-7b6cd5cbc5-b2tgj\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-controller-manager-minikube\": the object has been modified; please apply your changes to the latest version and try again]; queuing for retry" logger="UnhandledError"
I0926 13:26:39.323305       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
E0926 13:26:56.315565       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{coredns-66bc5c9577-djfsq.1868d7f8cd8f0118  kube-system   9912 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:coredns-66bc5c9577-djfsq,UID:e3794b62-ff17-44e3-a0d9-9c5839df2186,APIVersion:v1,ResourceVersion:3894,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2025-09-26 13:25:18 +0000 UTC,LastTimestamp:2025-09-26 13:26:09.923134196 +0000 UTC m=+6911.526757051,Count:3,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
E0926 13:28:12.110396       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: Get \"https://192.168.58.2:8443/api\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError"
I0926 13:28:12.111729       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="Get \"https://192.168.58.2:8443/api\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0926 13:28:25.025857       1 reflector.go:205] "Failed to watch" err="volumeattributesclasses.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"volumeattributesclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttributesClass"
E0926 13:28:25.025924       1 reflector.go:205] "Failed to watch" err="ingresses.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"ingresses\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Ingress"
I0928 01:07:11.171434       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0928 01:07:11.481303       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E0928 01:17:14.198039       1 replica_set.go:587] "Unhandled Error" err="sync \"prod/backend-6575c764b5\" failed with Unauthorized" logger="UnhandledError"


==> kube-proxy [a5bd97d1de15] <==
I0917 03:26:57.883708       1 server_linux.go:53] "Using iptables proxy"
I0917 03:26:58.243918       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0917 03:26:58.345471       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0917 03:26:58.345566       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.58.2"]
E0917 03:26:58.345733       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0917 03:26:58.413459       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0917 03:26:58.413526       1 server_linux.go:132] "Using iptables Proxier"
I0917 03:26:58.428812       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0917 03:26:58.432327       1 server.go:527] "Version info" version="v1.34.0"
I0917 03:26:58.432816       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 03:26:58.451858       1 config.go:200] "Starting service config controller"
I0917 03:26:58.451888       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0917 03:26:58.451919       1 config.go:106] "Starting endpoint slice config controller"
I0917 03:26:58.451924       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0917 03:26:58.451951       1 config.go:403] "Starting serviceCIDR config controller"
I0917 03:26:58.451958       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0917 03:26:58.456350       1 config.go:309] "Starting node config controller"
I0917 03:26:58.456634       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0917 03:26:58.456848       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0917 03:26:58.555839       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0917 03:26:58.555853       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0917 03:26:58.565298       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [d8e33f985dab] <==
I0923 20:06:05.166586       1 server_linux.go:53] "Using iptables proxy"
I0923 20:06:06.565534       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0923 20:06:06.765852       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0923 20:06:06.765897       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.58.2"]
E0923 20:06:06.765974       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0923 20:06:07.419253       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0923 20:06:07.419302       1 server_linux.go:132] "Using iptables Proxier"
I0923 20:06:07.431830       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0923 20:06:07.527246       1 server.go:527] "Version info" version="v1.34.0"
I0923 20:06:07.527318       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0923 20:06:07.546866       1 config.go:200] "Starting service config controller"
I0923 20:06:07.546888       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0923 20:06:07.546901       1 config.go:106] "Starting endpoint slice config controller"
I0923 20:06:07.546905       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0923 20:06:07.546937       1 config.go:403] "Starting serviceCIDR config controller"
I0923 20:06:07.546941       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0923 20:06:07.550244       1 config.go:309] "Starting node config controller"
I0923 20:06:07.550434       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0923 20:06:07.550586       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0923 20:06:07.647173       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0923 20:06:07.647390       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0923 20:06:07.747569       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [45870de4c37d] <==
E0917 03:25:15.984078       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0917 03:25:15.993704       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0917 03:25:16.039938       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0917 03:25:16.089669       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0917 03:25:16.180789       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0917 03:25:16.203150       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0917 03:25:16.233830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0917 03:25:16.366068       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0917 03:25:16.418159       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0917 03:25:16.434512       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0917 03:25:16.545497       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0917 03:25:16.608468       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0917 03:25:16.627683       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0917 03:25:16.745313       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0917 03:25:16.995308       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0917 03:25:19.613368       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0917 03:25:19.638381       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0917 03:25:19.718625       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0917 03:25:20.130133       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0917 03:25:20.217269       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0917 03:25:20.386683       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0917 03:25:20.476851       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0917 03:25:20.580778       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0917 03:25:20.942188       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0917 03:25:20.943332       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0917 03:25:21.171805       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0917 03:25:21.253989       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0917 03:25:21.586151       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0917 03:25:21.917536       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0917 03:25:21.929263       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0917 03:25:22.063503       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0917 03:25:22.179350       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0917 03:25:22.256764       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0917 03:25:22.302954       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0917 03:25:27.992389       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0917 03:25:28.240092       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0917 03:25:29.169273       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0917 03:25:29.327518       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0917 03:25:29.611320       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0917 03:25:29.691702       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0917 03:25:30.324112       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0917 03:25:30.523503       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0917 03:25:31.231153       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0917 03:25:31.249312       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0917 03:25:31.398036       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0917 03:25:32.178554       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0917 03:25:32.200684       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0917 03:25:32.314861       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0917 03:25:32.353785       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0917 03:25:32.742693       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0917 03:25:33.827410       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0917 03:25:34.187902       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0917 03:25:34.631442       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0917 03:25:42.222812       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0917 03:25:46.343803       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0917 03:26:35.472132       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 11:38:42.396213       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0917 11:38:42.410126       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I0917 11:38:42.410235       1 server.go:265] "[graceful-termination] secure server is exiting"
E0917 11:38:42.412243       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [60204e0b551b] <==
I0923 20:05:14.766669       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0923 20:05:14.766761       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0923 20:05:14.768220       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0923 20:05:14.768382       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0923 20:05:14.768466       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0923 20:05:14.768948       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0923 20:05:14.769081       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0923 20:05:14.769110       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0923 20:05:14.769205       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0923 20:05:14.769602       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0923 20:05:14.770269       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0923 20:05:14.770416       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0923 20:05:14.770502       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/resourceslices?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0923 20:05:14.770717       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/deviceclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0923 20:05:14.770936       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0923 20:05:14.771042       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/resourceclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0923 20:05:14.771135       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0923 20:05:14.771153       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0923 20:05:14.771258       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0923 20:05:14.771329       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0923 20:05:15.580915       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0923 20:05:15.586845       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0923 20:05:15.613241       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/resourceclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0923 20:05:15.653740       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0923 20:05:15.732912       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0923 20:05:15.736364       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0923 20:05:15.761537       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0923 20:05:15.808190       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0923 20:05:15.821068       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0923 20:05:15.841812       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0923 20:05:15.931562       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/resourceslices?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0923 20:05:16.009127       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0923 20:05:16.024925       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0923 20:05:16.045583       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0923 20:05:16.143852       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0923 20:05:16.173910       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0923 20:05:16.177339       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0923 20:05:16.265363       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0923 20:05:16.347830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: Get \"https://192.168.58.2:8443/apis/resource.k8s.io/v1/deviceclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0923 20:05:19.740299       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0923 20:05:19.740404       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0923 20:05:19.740450       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0923 20:05:19.740556       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0923 20:05:19.740692       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0923 20:05:19.740739       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0923 20:05:19.740777       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0923 20:05:19.740822       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0923 20:05:19.740861       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0923 20:05:19.740901       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0923 20:05:19.740934       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0923 20:05:19.740947       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0923 20:05:19.740957       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0923 20:05:19.741025       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0923 20:05:19.741125       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0923 20:05:19.741197       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0923 20:05:19.741466       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0923 20:05:19.741499       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0923 20:05:19.806075       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0923 20:05:24.335081       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0926 13:28:24.756569       1 reflector.go:205] "Failed to watch" err="csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot watch resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"


==> kubelet <==
Sep 28 01:52:00 minikube kubelet[1103]: I0928 01:51:59.262569    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-6575c764b5-m64x4" secret="" err="secret \"regcred\" not found"
Sep 28 01:52:09 minikube kubelet[1103]: E0928 01:52:09.818258    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:52:09 minikube kubelet[1103]: E0928 01:52:09.818371    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13234913 maxSize=10485760
Sep 28 01:52:09 minikube kubelet[1103]: E0928 01:52:09.864340    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="9.821s"
Sep 28 01:52:12 minikube kubelet[1103]: I0928 01:52:12.115644    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-6846b7747f-bfbbw" secret="" err="secret \"regcred\" not found"
Sep 28 01:52:13 minikube kubelet[1103]: E0928 01:52:13.191936    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.296s"
Sep 28 01:52:17 minikube kubelet[1103]: E0928 01:52:17.300004    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:52:17 minikube kubelet[1103]: E0928 01:52:17.300441    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13253166 maxSize=10485760
Sep 28 01:52:19 minikube kubelet[1103]: I0928 01:52:19.866927    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-6575c764b5-d9ddg" secret="" err="secret \"regcred\" not found"
Sep 28 01:52:27 minikube kubelet[1103]: E0928 01:52:27.315702    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:52:27 minikube kubelet[1103]: E0928 01:52:27.315825    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13282772 maxSize=10485760
Sep 28 01:52:31 minikube kubelet[1103]: E0928 01:52:31.193611    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.253s"
Sep 28 01:52:35 minikube kubelet[1103]: E0928 01:52:35.982785    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.031s"
Sep 28 01:52:38 minikube kubelet[1103]: E0928 01:52:38.058916    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:52:38 minikube kubelet[1103]: E0928 01:52:38.058990    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13326393 maxSize=10485760
Sep 28 01:52:49 minikube kubelet[1103]: E0928 01:52:49.458521    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="5.248s"
Sep 28 01:52:52 minikube kubelet[1103]: E0928 01:52:52.202764    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:52:52 minikube kubelet[1103]: E0928 01:52:52.203365    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13384442 maxSize=10485760
Sep 28 01:52:53 minikube kubelet[1103]: I0928 01:52:53.329316    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-6846b7747f-4ndkg" secret="" err="secret \"regcred\" not found"
Sep 28 01:53:03 minikube kubelet[1103]: E0928 01:53:03.700062    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="7.587s"
Sep 28 01:53:03 minikube kubelet[1103]: E0928 01:53:03.879880    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:03 minikube kubelet[1103]: E0928 01:53:03.880224    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13428338 maxSize=10485760
Sep 28 01:53:05 minikube kubelet[1103]: E0928 01:53:05.275019    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.368s"
Sep 28 01:53:14 minikube kubelet[1103]: I0928 01:53:14.840774    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-6575c764b5-m64x4" secret="" err="secret \"regcred\" not found"
Sep 28 01:53:19 minikube kubelet[1103]: E0928 01:53:19.510637    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:19 minikube kubelet[1103]: E0928 01:53:19.510764    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13518552 maxSize=10485760
Sep 28 01:53:19 minikube kubelet[1103]: I0928 01:53:19.544950    1103 scope.go:117] "RemoveContainer" containerID="7df641e3dd14f5273e5e8165395df193bb4fa9e787d3f72bece1a57f18241923"
Sep 28 01:53:19 minikube kubelet[1103]: I0928 01:53:19.545521    1103 scope.go:117] "RemoveContainer" containerID="e43213248791c354c841a5eace0a1c53aa861c197e251efbc6fda8c2882edf50"
Sep 28 01:53:25 minikube kubelet[1103]: E0928 01:53:25.692576    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:25 minikube kubelet[1103]: E0928 01:53:25.692690    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13538430 maxSize=10485760
Sep 28 01:53:36 minikube kubelet[1103]: E0928 01:53:36.200701    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:36 minikube kubelet[1103]: E0928 01:53:36.201247    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13567277 maxSize=10485760
Sep 28 01:53:46 minikube kubelet[1103]: I0928 01:53:46.843907    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-6846b7747f-bfbbw" secret="" err="secret \"regcred\" not found"
Sep 28 01:53:47 minikube kubelet[1103]: E0928 01:53:47.874840    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:47 minikube kubelet[1103]: E0928 01:53:47.874968    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13598177 maxSize=10485760
Sep 28 01:53:52 minikube kubelet[1103]: E0928 01:53:52.898740    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.027s"
Sep 28 01:53:56 minikube kubelet[1103]: I0928 01:53:56.075690    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-6575c764b5-d9ddg" secret="" err="secret \"regcred\" not found"
Sep 28 01:53:59 minikube kubelet[1103]: E0928 01:53:58.744033    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:53:59 minikube kubelet[1103]: E0928 01:53:58.744103    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13620180 maxSize=10485760
Sep 28 01:54:03 minikube kubelet[1103]: E0928 01:54:03.411316    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.151s"
Sep 28 01:54:03 minikube kubelet[1103]: I0928 01:54:03.411587    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-6846b7747f-4ndkg" secret="" err="secret \"regcred\" not found"
Sep 28 01:54:08 minikube kubelet[1103]: E0928 01:54:08.824891    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:54:08 minikube kubelet[1103]: E0928 01:54:08.825742    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13670667 maxSize=10485760
Sep 28 01:54:21 minikube kubelet[1103]: E0928 01:54:21.723641    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:54:21 minikube kubelet[1103]: E0928 01:54:21.723833    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13786493 maxSize=10485760
Sep 28 01:54:35 minikube kubelet[1103]: E0928 01:54:35.726013    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="7.892s"
Sep 28 01:54:36 minikube kubelet[1103]: E0928 01:54:36.864218    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:54:36 minikube kubelet[1103]: E0928 01:54:36.864368    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13864656 maxSize=10485760
Sep 28 01:54:37 minikube kubelet[1103]: I0928 01:54:37.667418    1103 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-7ldqn" podStartSLOduration=383.713406924 podStartE2EDuration="12m43.667375055s" podCreationTimestamp="2025-09-28 01:41:54 +0000 UTC" firstStartedPulling="2025-09-28 01:46:52.922483113 +0000 UTC m=+11154.315312422" lastFinishedPulling="2025-09-28 01:53:36.745844718 +0000 UTC m=+11534.269280553" observedRunningTime="2025-09-28 01:54:12.861465188 +0000 UTC m=+11568.382382789" watchObservedRunningTime="2025-09-28 01:54:37.667375055 +0000 UTC m=+11591.299531671"
Sep 28 01:54:42 minikube kubelet[1103]: E0928 01:54:42.779193    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:54:42 minikube kubelet[1103]: E0928 01:54:42.779774    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13906180 maxSize=10485760
Sep 28 01:54:44 minikube kubelet[1103]: I0928 01:54:44.966258    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-6575c764b5-m64x4" secret="" err="secret \"regcred\" not found"
Sep 28 01:54:54 minikube kubelet[1103]: E0928 01:54:54.893681    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:54:54 minikube kubelet[1103]: E0928 01:54:54.894249    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=13962703 maxSize=10485760
Sep 28 01:55:01 minikube kubelet[1103]: E0928 01:55:01.074955    1103 kubelet.go:2617] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.216s"
Sep 28 01:55:04 minikube kubelet[1103]: E0928 01:55:04.781185    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:55:04 minikube kubelet[1103]: E0928 01:55:04.781272    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=14001504 maxSize=10485760
Sep 28 01:55:13 minikube kubelet[1103]: I0928 01:55:13.731963    1103 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-6846b7747f-bfbbw" secret="" err="secret \"regcred\" not found"
Sep 28 01:55:14 minikube kubelet[1103]: E0928 01:55:14.801169    1103 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f"
Sep 28 01:55:14 minikube kubelet[1103]: E0928 01:55:14.801266    1103 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log\": failed to reopen container log \"8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8a643aa3bf492309e6a92b631fabc997890c924fcd6199d15bad34d98717416f" path="/var/log/pods/kube-system_etcd-minikube_2984930d46b1e7a87dd14a2b8d7597e0/etcd/1.log" currentSize=14016921 maxSize=10485760


==> storage-provisioner [7df641e3dd14] <==
command /bin/bash -c "docker logs --tail 60 7df641e3dd14" failed with error: /bin/bash -c "docker logs --tail 60 7df641e3dd14": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 7df641e3dd14


==> storage-provisioner [e43213248791] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0003d40e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003d40e0, 0x18b3d60, 0xc0002140c0, 0x1, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003d40e0, 0x3b9aca00, 0x0, 0x17a0501, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003d40e0, 0x3b9aca00, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 64 [sync.Cond.Wait, 32 minutes]:
sync.runtime_notifyListWait(0xc0004acd10, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004acd00)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0004d4a20, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc000500500, 0x18e5530, 0xc00011f980, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0003d4260)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003d4260, 0x18b3d60, 0xc0002140f0, 0x1, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003d4260, 0x3b9aca00, 0x0, 0x17a0501, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003d4260, 0x3b9aca00, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 97 [sync.Cond.Wait, 32 minutes]:
sync.runtime_notifyListWait(0xc0004acd50, 0x1)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004acd40)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0004d4ba0, 0x0, 0x0, 0x1590200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc000500500, 0x18e5530, 0xc00011f980, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0003d4280)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003d4280, 0x18b3d60, 0xc0005e1a40, 0x1, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003d4280, 0x3b9aca00, 0x0, 0xc0003d7201, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003d4280, 0x3b9aca00, 0xc0000a8000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

